<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="cn"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://1587causalai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://1587causalai.github.io/" rel="alternate" type="text/html" hreflang="cn"/><updated>2025-04-29T12:14:46+08:00</updated><id>https://1587causalai.github.io/feed.xml</id><title type="html">Teach AI Causes and Effects</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Qwen 3: Alibaba Cloud’s Latest LLM Breakthrough (and a Workflow Test)</title><link href="https://1587causalai.github.io/blog/2025/qwen-3-alibaba-cloud-llm-breakthrough/" rel="alternate" type="text/html" title="Qwen 3: Alibaba Cloud’s Latest LLM Breakthrough (and a Workflow Test)"/><published>2025-04-29T00:00:00+08:00</published><updated>2025-04-29T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/qwen-3-alibaba-cloud-llm-breakthrough</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/qwen-3-alibaba-cloud-llm-breakthrough/"><![CDATA[<blockquote> <p><strong>核心关注：优化博客工作流 (Core Focus: Optimizing the Blogging Workflow)</strong></p> <p>这篇文章承载了双重目标。表面上，它介绍了阿里巴巴云最新的 Qwen 3 大语言模型。但<strong>更深层次，也是本次写作的核心驱动力，在于对我的博客创作与发布流程进行一次实战检验与优化。</strong></p> <p>长期以来，我一直感受到传统博客流程的繁琐，尤其是在处理文件名、元数据（Front Matter）等细节上耗时费力，与快速分享想法的初衷相悖。为了突破这一瓶颈，我近期与 AI 合作，<strong>系统性地梳理并固化了一套博客文章的格式规范</strong>（详见 <code class="language-plaintext highlighter-rouge">_journal/blogmeta_rules.md</code>）。</p> <p>这篇关于 Qwen 3 的技术文章，正是应用这套新规范的<strong>首个产物和关键测试案例</strong>。我们严格遵循了预设规则来创建文件、生成元数据（全部借助 AI），并组织内容。其主要目的就是<strong>验证这套标准化、半自动化的流程能否显著提升从想法到发布的效率和流畅度</strong>。</p> <p>因此，在阅读下文关于 Qwen 3 的介绍时，请理解其内容可能因侧重流程验证而有待进一步打磨。<strong>本文更重要的价值在于展示和反思这一”内容创作即流程优化”的实践</strong>，希望能为同样寻求高效知识分享方式的朋友带来一点启发。<strong>工作流本身，与内容同等重要。</strong></p> </blockquote> <h2 id="引言">引言</h2> <p>2025 年 4 月 29 日，阿里巴巴云发布了 Qwen 3，这是一个标志性的大型语言模型（LLM）系列，代表了自然语言处理（NLP）领域的重大进步。Qwen 3 构建在 Qwen 2.5 的成功基础上，通过创新的模型架构、扩展的训练数据和优化的后训练流程，显著提升了性能。本文将深入探讨 Qwen 3 的功能、性能指标及其在 AI 生态系统中的定位，为开发者和研究人员提供全面的参考。</p> <h2 id="模型概览">模型概览</h2> <p>Qwen 3 系列包括密集模型和专家混合（MoE）模型，参数规模从 0.6B 到 235B，满足从轻量级设备到高性能计算的多样化需求。所有模型均在 Apache 2.0 许可下开源，促进了 AI 社区的协作与创新。以下是主要模型的概况：</p> <table> <thead> <tr> <th>模型名称</th> <th>类型</th> <th>总参数</th> <th>激活参数</th> <th>上下文长度</th> </tr> </thead> <tbody> <tr> <td>Qwen3-235B-A22B</td> <td>MoE</td> <td>2350 亿</td> <td>220 亿</td> <td>128K 令牌</td> </tr> <tr> <td>Qwen3-30B-A3B</td> <td>MoE</td> <td>300 亿</td> <td>30 亿</td> <td>128K 令牌</td> </tr> <tr> <td>Qwen3-32B</td> <td>密集</td> <td>320 亿</td> <td>320 亿</td> <td>128K 令牌</td> </tr> <tr> <td>Qwen3-14B</td> <td>密集</td> <td>140 亿</td> <td>140 亿</td> <td>128K 令牌</td> </tr> <tr> <td>Qwen3-8B</td> <td>密集</td> <td>80 亿</td> <td>80 亿</td> <td>128K 令牌</td> </tr> <tr> <td>Qwen3-4B</td> <td>密集</td> <td>40 亿</td> <td>40 亿</td> <td>32K 令牌</td> </tr> <tr> <td>Qwen3-1.7B</td> <td>密集</td> <td>17 亿</td> <td>17 亿</td> <td>32K 令牌</td> </tr> <tr> <td>Qwen3-0.6B</td> <td>密集</td> <td>6 亿</td> <td>6 亿</td> <td>32K 令牌</td> </tr> </tbody> </table> <p>旗舰模型 Qwen3-235B-A22B 以其大规模参数和高效的 MoE 架构脱颖而出，而较小的模型如 Qwen3-4B 则在性能与资源需求之间取得了平衡。例如，Qwen3-4B 的性能可媲美 Qwen2.5-72B-Instruct，显示了其高效性。</p> <p>Qwen 3 模型已在 Hugging Face、ModelScope 和 Kaggle 上发布，推荐使用 SGLang 或 vLLM 进行部署，Ollama 和 LMStudio 则适合本地使用。</p> <h2 id="关键特性">关键特性</h2> <p>Qwen 3 引入了多项创新功能，使其在众多大型语言模型中独树一帜：</p> <h3 id="混合思维模式">混合思维模式</h3> <p>Qwen 3 支持两种操作模式：</p> <ul> <li><strong>思考模式 (Think Mode):</strong> 针对需要逐步推理的复杂任务，如数学问题求解、编码或逻辑推理，模型会生成详细的思维链（Chain-of-Thought, CoT）。</li> <li><strong>非思考模式 (No-Think Mode):</strong> 优化快速响应，适合通用聊天或简单查询，提高效率。</li> </ul> <p>用户可通过提示中的 <code class="language-plaintext highlighter-rouge">/think</code> 和 <code class="language-plaintext highlighter-rouge">/no_think</code> 动态切换模式，或在代码中设置 <code class="language-plaintext highlighter-rouge">enable_thinking=True/False</code>。这一功能显著提升了模型在不同场景下的适应性。</p> <h3 id="多语言支持">多语言支持</h3> <p>Qwen 3 支持超过 119 种语言和方言，涵盖中文、英文、法语、西班牙语等主要语言，以及多种区域性语言。这使其成为多语言指令遵循和翻译任务的理想选择。</p> <h3 id="扩展上下文长度">扩展上下文长度</h3> <p>大型模型（如 Qwen3-235B-A22B 和 Qwen3-32B）支持高达 128,000 令牌的上下文长度，适合处理长文档、复杂对话或需要长期记忆的任务。较小模型（如 Qwen3-4B）支持 32,000 令牌，依然优于许多同类模型。</p> <h3 id="代理功能">代理功能</h3> <p>Qwen 3 集成了强大的代理功能，可与外部工具和 API 交互。例如，通过 Qwen-Agent，模型支持代码解释器和多模态内容处理（MCP），适用于自动化工作流和复杂任务执行。</p> <h2 id="预训练与后训练">预训练与后训练</h2> <p>Qwen 3 的预训练数据高达 36 万亿令牌，覆盖 119 种语言，远超 Qwen2.5 的 18 万亿令牌。训练分为三个阶段：</p> <ol> <li><strong>阶段 1:</strong> 超过 30 万亿令牌，4K 上下文，奠定基础。</li> <li><strong>阶段 2:</strong> 增加 5 万亿令牌，聚焦 STEM、编码和推理数据。</li> <li><strong>阶段 3:</strong> 高质量长上下文数据，扩展至 32K 上下文。</li> </ol> <p>后训练采用四阶段流程，包括长 CoT 初始化、基于推理的强化学习（RL）、思维模式融合和通用 RL，优化了模型在指令遵循、创意写作和多轮对话中的表现。</p> <h2 id="性能评估">性能评估</h2> <p>Qwen 3 的性能通过多项基准测试得到验证，显示其在编码、数学和通用能力方面的卓越表现。以下是 Qwen3-235B-A22B 在关键基准测试中的得分，与其他顶级模型的对比：</p> <table> <thead> <tr> <th>基准测试</th> <th>Qwen3-235B-A22B</th> <th>DeepSeek-R1</th> <th>o1</th> <th>Grok-3</th> <th>Gemini-2.5-Pro</th> </tr> </thead> <tbody> <tr> <td>Arena-Hard</td> <td>95.6</td> <td>92.1</td> <td>92.2</td> <td>-</td> <td>-</td> </tr> <tr> <td>AIME24</td> <td>81.5</td> <td>81.3</td> <td>78.8</td> <td>-</td> <td>-</td> </tr> <tr> <td>LiveCodeBench</td> <td>70.7</td> <td>64.3</td> <td>67.6</td> <td>-</td> <td>-</td> </tr> <tr> <td>CodeR</td> <td>82.6</td> <td>79.9</td> <td>80.1</td> <td>-</td> <td>-</td> </tr> <tr> <td>Aider</td> <td>61.8</td> <td>50.9</td> <td>52.3</td> <td>-</td> <td>-</td> </tr> <tr> <td>LIVEbench</td> <td>77.1</td> <td>67.7</td> <td>70.4</td> <td>-</td> <td>-</td> </tr> <tr> <td>BCLE</td> <td>70.8</td> <td>46.8</td> <td>48.6</td> <td>-</td> <td>-</td> </tr> <tr> <td>MuLTI-CLE</td> <td>71.9</td> <td>67.7</td> <td>69.7</td> <td>-</td> <td>-</td> </tr> </tbody> </table> <p>这些数据表明，Qwen3-235B-A22B 在大多数基准测试中超越了 DeepSeek-R1 和 o1，尤其是在 Aider（61.8 vs. 50.9）和 BCLE（70.8 vs. 46.8）等任务中优势明显。Arena-Hard 得分 95.6 反映了其在复杂任务中的强大指令遵循能力，而 LiveCodeBench 和 CodeR 的高分凸显了其编码能力。</p> <p>此外，Qwen3-30B-A3B 尽管激活参数较少，仍优于 QwQ-32B，显示了 MoE 架构的高效性。Qwen3-4B 的性能甚至可媲美 Qwen2.5-72B-Instruct，表明小型模型在资源受限环境中的潜力。</p> <p>需要注意的是，部分基准数据来自社交媒体分享（如 X 平台），可能需要进一步验证以确保准确性。然而……</p> <h2 id="应用场景">应用场景</h2> <p>Qwen 3 的多功能性使其适用于众多行业和场景：</p> <h3 id="软件开发">软件开发</h3> <p>凭借出色的编码能力，Qwen 3 可协助开发者编写、调试和优化代码。例如，在 LiveCodeBench 和 CodeR 测试中的高分表明其在生成和执行功能性代码方面的可靠性，适合用于自动化代码审查或生成复杂算法。</p> <h3 id="教育">教育</h3> <p>Qwen 3 在数学和逻辑推理方面的能力使其成为教育领域的理想工具。它可以作为智能辅导系统，辅助学生解决数学问题，或为编程课程提供实时反馈。AIME24 得分 81.5 证明了其在高级数学任务中的实力。</p> <h3 id="客户服务">客户服务</h3> <p>在非思考模式下，Qwen 3 可驱动高效的聊天机器人，提供快速、准确的客户支持。其多语言支持确保全球用户能够以母语获得帮助，提升用户体验。</p> <h3 id="多语言通信">多语言通信</h3> <p>支持超过 119 种语言，Qwen 3 可用于翻译服务、多语言内容生成和跨文化交流。例如，它可以实时翻译技术文档或生成多语言营销内容。</p> <h3 id="研究与分析">研究与分析</h3> <p>研究人员可利用 Qwen 3 进行自然语言处理任务，如文本摘要、数据解释或生成研究假设。其长上下文支持（高达 128K 令牌）使其能够处理大型数据集或复杂科学文献。</p> <h2 id="未来展望">未来展望</h2> <p>Qwen 3 的发布不仅是技术成就，也是迈向通用人工智能（AGI）和超级人工智能（ASI）的重要一步。阿里巴巴云计划进一步扩展数据和模型规模、延长上下文长度、扩展多模态功能，并通过环境反馈改进强化学习，以增强长程推理能力。这些努力旨在打造更智能、更通用的 AI 系统。</p> <h2 id="结论">结论</h2> <p>Qwen 3 是大型语言模型领域的里程碑，其混合思维模式、广泛的语言支持和顶级基准性能使其成为领先的 AI 模型。无论是开发者、教育工作者还是企业，Qwen 3 都提供了无与伦比的灵活性和能力。随着阿里巴巴云持续推动 AI 创新，Qwen 3 无疑将在塑造智能系统未来中发挥关键作用。</p>]]></content><author><name></name></author><category term="AI-Research"/><category term="Workflow"/><category term="ai"/><category term="llm"/><category term="qwen"/><category term="alibaba cloud"/><category term="nlp"/><category term="benchmark"/><category term="moe"/><category term="release"/><category term="workflow"/><category term="blogging"/><category term="automation"/><category term="meta"/><summary type="html"><![CDATA[Testing a streamlined blog creation workflow by documenting the Qwen 3 release, following predefined metadata rules. The focus is on the process, not just the content.]]></summary></entry><entry><title type="html">重构你的大脑</title><link href="https://1587causalai.github.io/blog/2025/mind-creation/" rel="alternate" type="text/html" title="重构你的大脑"/><published>2025-03-22T00:00:00+08:00</published><updated>2025-03-22T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/mind-creation</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/mind-creation/"><![CDATA[<p>最近读了 Scott Adams 的 <strong>《Reframe Your Brain: The User Interface for Happiness and Success》（重构你的大脑：通往幸福和成功的用户界面）</strong>。 Scott Adams 可能大家更熟悉的是他的漫画作品《呆伯特 (Dilbert)》，但他同时也是一位观察敏锐的思想家，尤其擅长洞察人类行为和说服力。《Reframe Your Brain》这本书，探讨如何主动重塑心智模式</p> <p>这本书实在是太好了, 有非常多的新颖的洞见, 重新设定框架, 主动把现实理解成一个对你更有用的故事(认知重构, 心智对身体有巨大影响力), 我太需要这本书了哈哈. 尤其是在这个人工智能时代, 我们过往的很多习惯认知都需要被重构, 需要重新思考如何和这个世界打交道.</p> <p>这本书给了一个非常好的三层心智重构框架:</p> <ul> <li>你的情绪是可以自己选择的。</li> <li>与其说现实是你看到的和感受到的东西, 不如想象你周围的事物都是虚拟的现象</li> <li>你自己的各种感受和想法其实都是可以完全忽略的</li> </ul> <p>同时，书中提供了许多充满辩证和智慧的实用建议, 比如:</p> <ul> <li>与其说要找到你自己(find yourself), 不如说你要成为自己的作者(author yourself).</li> <li>使用尴尬消除 ego</li> <li>能量管理而不是时间管理</li> <li>你真正要学的不是社交技巧，而是理解他人: <ul> <li>人其实很容易被操作摆布, 所以有给 “foo 理由” 技巧</li> <li>人永远关心的是自己, 所以我们选择问别人他们自己的问题, 准备一个生活化的“自我介绍” (而非简历), 表演是最高的技术.</li> <li>你需要知道人们真正需要的是什么.</li> </ul> </li> <li>… (书中还有更多精彩内容，此处省略)</li> </ul> <p>书中还有更多精彩内容，这里就不一一列举了。总而言之，这本书我觉得我需要反复读上十遍，是一本既能打开思路又实用性极强的书，强烈推荐。</p> <p>TODO:</p> <ul> <li>后续学十遍, …</li> <li>有另外一本书叫做: 重新思考(Adam Grant), 我比较好奇这两本书的关系 <ul> <li>《Reframe Your Brain》 更侧重于个体层面心智模式的重塑和情绪管理，目标在于个人成长和幸福感提升；而 《重新思考》 则更侧重于思维方式的迭代和认知灵活性，目标在于提升决策质量和应对复杂环境的能力。两本书各有侧重，都非常值得一读。</li> </ul> </li> </ul> ]]></content><author><name></name></author><category term="thinking"/><category term="book,"/><category term="thinking,"/><category term="认知重构,"/><category term="心智模式"/><summary type="html"><![CDATA[好书分享, Scott Adams, Reframe Your Brain-The User Interface for Happiness and Success]]></summary></entry><entry><title type="html">从完成投篮动作到最小必要改动渐进式开发原则</title><link href="https://1587causalai.github.io/blog/2025/basketball/" rel="alternate" type="text/html" title="从完成投篮动作到最小必要改动渐进式开发原则"/><published>2025-03-18T00:00:00+08:00</published><updated>2025-03-18T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/basketball</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/basketball/"><![CDATA[<h2 id="一篮球场上的顿悟替代目标的力量">一、篮球场上的顿悟：替代目标的力量</h2> <p>今天，我在篮球场上经历了一场颇具启发性的投篮训练，意外发现了一个有趣的现象：<strong>看似不再追求命中率，却获得了最高的命中率</strong>。通过将目标从”投进球”调整为”尽快完成投篮动作”，我的命中率从30%飙升至70%。这一经历让我开始反思，无论是篮球训练、提示词工程，还是人生目标的设定，找到合适的替代目标可能是解决复杂问题的关键。</p> <div style="text-align: center;"> <img src="https://s2.loli.net/2025/03/19/YmONaVps3Rjv5DH.png" alt="basketball shooting" width="300" height="auto"/> <p>*篮球投篮的瞬间，专注于快速完成投篮动作而非命中*</p> </div> <p>刚开始投篮时，我的目标很直接——把球投进篮筐。然而，无论我如何努力，命中率始终不高，徘徊在30%左右，有时甚至低至20%。我尝试调整心态，甚至通过冥想让自己不去执着于”投进球”的念头，但效果依然不佳，命中率停留在30%。</p> <p>后来，我决定换个思路，不再直接追求”投进球”这个最终目标，而是寻找一个<strong>替代目标（Proxy Goal）</strong>。我的第一个尝试是”完成一个好的投篮动作”，希望通过优化动作来提升命中率。然而，这个目标并没有带来明显改善，命中率依然没有起色。</p> <p>直到我尝试了另一个替代目标——”<strong>尽快完成投篮动作</strong>“，奇迹发生了。我的命中率从30%飙升到了70%。为了验证这个效果，我进行了两次10个球的投篮训练，每次都稳定达到了70%的命中率。更令人惊喜的是，在这两次快速投篮训练后，我重新将目标切换回”投进球”，命中率也提升到了60%，远超之前的水平。</p> <p>这个经历让我最大的感悟是：<strong>看似不再追求命中率，却获得了最高的命中率</strong>。直接优化”投进球”这个真实目标时，我常常事与愿违，因为影响命中率的因素太多——手感、姿势、力度、角度，甚至心理状态——反馈信号太过模糊，难以指导我的改进。而”尽快完成投篮动作”作为一个替代目标，给了我一个清晰、可控的方向，专注于过程反而间接提升了整体表现。</p> <h2 id="二替代目标的普遍性辅助线与因果智慧">二、替代目标的普遍性：辅助线与因果智慧</h2> <p>这次篮球训练让我深刻体会到替代目标的价值。<strong>当真实目标受众多因素影响，反馈信号较弱时，一个合适的替代目标可以成为更易于优化的中间指标</strong>。它就像几何题中那条绝妙的辅助线，或者物理问题中一个恰到好处的坐标系，虽然不是最终答案，却能帮助我们更高效地接近目标。</p> <div style="text-align: center;"> <img src="https://s2.loli.net/2025/03/19/6ojEfCtVXJWTwzp.png" alt="surrogate goal" width="300" height="auto"/> <p>*找到替代指标是一种创造力*</p> </div> <p>在篮球的例子中，”尽快完成投篮动作”与”投进球”之间存在很强的因果关系。快速完成动作让我减少了过多思考和犹豫，反而使投篮更自然流畅，最终提升了命中率。这种方法的核心在于，替代目标不仅简化了优化过程，还提供了更强的梯度反馈，让我能够逐步调整和改进。</p> <p>这种思维在其他领域同样适用。例如，在使用大语言模型开发服务时，我曾对模型说：”我要坚持<strong>最小必要改动渐进式开发原则</strong>。”这何尝不是一个绝佳的替代目标？开发中直接追求”完美代码”往往不现实，但通过每次只做最小的改动、测试效果、再逐步优化，我能更高效地接近最终目标。这种”最小必要改动”的策略，与篮球场上的”尽快完成投篮动作”异曲同工，都是通过聚焦于可控的中间步骤，间接提升整体表现。</p> <h2 id="三提示词工程设计替代目标的艺术">三、提示词工程：设计替代目标的艺术</h2> <p>什么是最好的提示词工程？这是我一直在思考的问题，而今天的篮球体验让我有了一个新的观点：<strong>提示词工程是设计替代目标的艺术，是将复杂任务拆解成合适中间步骤的艺术</strong>。</p> <p>在使用大语言模型（LLM）时，我们需要通过提示词（Prompt）引导模型生成符合预期的输出。但直接告诉模型”给我一个完美的答案”往往不够具体，效果有限。这时，我们需要设计一个替代目标，比如”先清晰地总结上下文信息”或”逐步回答问题的每个部分”。这些中间步骤就像篮球中的”尽快完成投篮动作”，通过聚焦于可控的子目标，间接提升最终输出的质量。</p> <p>更重要的是，设计替代目标需要人类发挥独特的作用。<strong>我们扮演着那条巧妙的辅助线的角色</strong>，需要对问题有高屋建瓴的洞察、深刻的因果智慧，以及灵光一现的创造力。只有这样，我们才能将复杂任务拆解为模型能够有效执行的中间步骤，从而充分发挥人工智能的潜力。</p> <h2 id="四提示词工程的层次从基本素养到高级技能">四、提示词工程的层次：从基本素养到高级技能</h2> <p>关于提示词工程，我认为它可以分为两个层次：</p> <ol> <li> <p><strong>基本素养：上下文管理</strong><br/> 提示词工程首先是一个上下文管理问题。确保为模型提供清晰的背景信息、明确的任务指令，这些都是提示词工程的基础。</p> </li> <li> <p><strong>高级技能：设计替代目标</strong><br/> 在此之上，设计替代目标是一种更高级的能力。它不仅需要技术层面的理解，还需要创造力和对问题的深刻洞察。比如，在一个复杂的问答任务中，我们可以先让模型”列出问题的关键点”，再逐步引导它”针对每个关键点生成详细回答”，最终合成完整的答案。这种拆解和设计的过程，正是提示词工程的艺术所在。</p> </li> </ol> <p>这种层次化的理解让我意识到，提示词工程不仅是技术工具，更是人类智慧与人工智能能力结合的桥梁。通过设计合适的替代目标，我们能让人工智能更好地服务于我们的需求。</p> <h2 id="五替代目标与人生目标追求过程的力量">五、替代目标与人生目标：追求过程的力量</h2> <p>这次篮球训练的领悟不仅让我在提示词工程上有所启发，还让我对人生目标的追求有了新的思考。<strong>当我执着于某个目标时，直接追求它可能是一个非常差的策略</strong>。真实世界的影响因素非常多，目标的达成需要太多条件的配合，多到脱离我的掌控，甚至让我的动作变形。</p> <p>在篮球训练中，我发现”尽快完成投篮动作”比直接追求”投进球”更有效。类似地，在人生中，<strong>将宏大的目标拆解为可控的中间步骤</strong>可能是更明智的选择。例如，面对一个长期职业目标，与其直接追求”成功”，不如专注于”每天完成该做的工作”或”持续提升某项技能”。这些替代目标不仅更易于管理，还能提供更清晰的反馈，帮助我逐步接近最终目标。</p> <p>这种思维方式提醒我，<strong>追求过程有时比直接追求结果更重要</strong>。正如在篮球训练中，专注于动作的完成度反而提升了命中率；在人生中，专注于每一步的努力和成长，往往能带来意想不到的成功。</p> <h2 id="六结语人类的独特价值与替代目标的力量">六、结语：人类的独特价值与替代目标的力量</h2> <p>通过这次篮球训练，我不仅提高了投篮命中率，更重要的是获得了一个全新的视角：<strong>替代目标是优化复杂任务的关键</strong>。无论是体育训练、人工智能开发，还是人生目标设定，找到合适的中间指标都能让我们事半功倍。</p> <p>在与人工智能的合作中，人类的价值无可替代。我们通过洞察力和创造力，设计出巧妙的替代目标，引导人工智能发挥其强大的能力。提示词工程不仅是上下文管理的工具，更是人类智慧与人工智能潜能结合的艺术。</p> <p>希望这个思考能启发更多人，让我们在追求个人目标和与人工智能合作时，找到更多”尽快完成投篮动作”式的奇妙替代目标，共同创造更大的价值。</p>]]></content><author><name></name></author><category term="thinking"/><category term="prompt-engineering"/><category term="AI"/><category term="思考"/><category term="basketball"/><summary type="html"><![CDATA[今天，我在篮球场上经历了一场颇具启发性的投篮训练，不仅让我对如何提高投篮命中率有了新的认识，还让我对如何给大语言模型喂提示词，甚至如何给作为智能体的自己喂提示词有了新的思考。]]></summary></entry><entry><title type="html">OpenAI 的陨落：灵魂已逝，技术壁垒坍塌，再见，CloseAI！</title><link href="https://1587causalai.github.io/blog/2025/openai-lost-soul/" rel="alternate" type="text/html" title="OpenAI 的陨落：灵魂已逝，技术壁垒坍塌，再见，CloseAI！"/><published>2025-02-20T00:00:00+08:00</published><updated>2025-02-20T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/openai-lost-soul</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/openai-lost-soul/"><![CDATA[<p>OpenAI 曾是人工智能领域的耀眼明星，其技术创新和影响力一度无人能及。然而，如今的 OpenAI 正逐渐失去往日的荣光，其衰落的原因可以归结为两点：一是失去了灵魂，二是技术壁垒的崩塌。本文将从多个角度深入剖析 OpenAI 的困境——灵魂人物 Ilya Sutskever 的离去、对中国用户的无底线封锁、deepseek 开源模式的崛起、挤牙膏式发布的短视策略，以及 Grok 和 Claude 等竞争对手的威胁。这些因素共同指向一个结论：OpenAI 的失败似乎已成定局，而 CEO Sam Altman 难逃其责。</p> <h2 id="灵魂人物陨落openai-的精神内核崩解">灵魂人物陨落：OpenAI 的精神内核崩解</h2> <p>伊利亚·苏茨克维 (Ilya Sutskever)，这位 OpenAI 的联合创始人兼首席科学家，是这家公司真正的灵魂人物。他的名字，与人工智能最前沿的突破紧密相连。他的离去，不仅仅是一位顶尖科学家的流失，更象征着 OpenAI 精神内核的崩塌。失去了 Sutskever，创新步伐显著放缓。一个失去灵魂的公司，如何能在 AI 领域的激烈竞争中继续称霸？ 一个没有灵魂的企业，注定走向平庸和衰落。</p> <h2 id="罪恶的歧视无底线封锁与侮辱中国用户">罪恶的歧视：无底线封锁与侮辱中国用户</h2> <p>OpenAI 对中国用户的封锁政策堪称自毁长城，其无底线行为令人愤怒。中国用户若想使用 OpenAI 的顶级付费服务（如 200 美元/月的 Pro 账号），需面对重重障碍：从注册时的手机号限制，到付费和登录的层层验证，其难度之高，堪称全球独一份！</p> <p>更令人发指的，是 OpenAI 对中国用户的公然侮辱。一位中国用户，历尽千辛万苦，成功订阅了每月 200 美元的 Pro 账号，却发现在中国大陆登录时，OpenAI 竟然使用弱智模型来糊弄他！同一账号，同一对话，在日本登录却能畅享所有先进模型！这种赤裸裸的歧视和侮辱，是对中国用户尊严的践踏，是任何有良知的人都无法容忍的！Grok 和 Claude 虽然也有地域限制，但绝不会如此卑劣地歧视用户。OpenAI 的这种无底线行为，彻底暴露了其价值观的沦丧和灵魂的缺失！</p> <h2 id="开源浪潮汹涌技术壁垒轰然倒塌">开源浪潮汹涌：技术壁垒轰然倒塌</h2> <p>OpenAI 一直将闭源视为核心竞争力，以此构建技术壁垒。然而，开源的力量如同滔天巨浪，正在无情地冲击着 OpenAI 的”技术堡垒”。DeepSeek 等开源项目的崛起，证明了开源模型完全可以媲美甚至超越闭源模型。开源已是大势所趋，是人工智能的未来！</p> <p>在开源浪潮的冲击下，OpenAI 引以为傲的技术壁垒正在轰然倒塌。当技术不再是秘密，当开源模型日益强大，OpenAI 的闭源模式将显得愈发落后和僵化。技术壁垒的瓦解，将彻底动摇 OpenAI 的根基。</p> <h2 id="挤牙膏式发布傲慢与短视的代名词">“挤牙膏”式发布：傲慢与短视的代名词</h2> <p>OpenAI 在产品发布上的”挤牙膏”策略，更是傲慢和短视的体现。与其推出成熟完整的产品，OpenAI 更倾向于零散发布新功能，试图以此吊住用户胃口。然而，这种做法适得其反，用户感受到的不是期待，而是厌恶和失望。他们需要的是稳定、高效的工具，而不是被迫适应频繁而琐碎的更新。这种短视策略不仅损害了用户体验，也让 OpenAI 在市场上的口碑和竞争力大幅下滑。</p> <h2 id="技术优势不再竞争对手强势崛起">技术优势不再：竞争对手强势崛起</h2> <p>OpenAI 曾经引以为傲的技术壁垒正在崩塌，竞争对手的崛起让其处境雪上加霜。xAI 的 Grok3 横空出世，登顶大模型竞技场；Claude 3 Sonnet 也在工程开发领域遥遥领先 OpenAI。越来越多的迹象表明，OpenAI 正在失去技术领先地位。Figure AI 机器人公司解除与 OpenAI 的合作，微软也减少了对 OpenAI 的独家支持，这些都预示着 OpenAI 的衰落已不可避免。</p> <h2 id="结语再见closeaisam-altman-必将为他的傲慢付出代价">结语：再见，CloseAI！Sam Altman 必将为他的傲慢付出代价！</h2> <p>OpenAI 的衰落可以归结为两个致命问题。首先，它失去了灵魂。Ilya Sutskever 的离职、对中国用户的歧视性封锁，以及 Sam Altman 领导下的战略失误，让 OpenAI 从一个充满理想的创新先锋沦为缺乏价值观的空壳。其次, Deepseek, Grok, Claude, Gemini 等竞争对手的已经崛起, 技术壁垒正在坍塌，OpenAI 的陨落已成定局。Sam Altman 的傲慢和短视，将 OpenAI 一步步推向深渊。曾经辉煌的 OpenAI，如今已沦为”CloseAI”，一个封闭、傲慢、歧视用户的”封闭人工智能”！</p> <p>再见，CloseAI！人工智能的未来，绝不属于 OpenAI 这样的”封闭帝国”，而将属于更加开放、包容、以人为本的开源世界！Sam Altman，你终将为你的傲慢和错误决策付出代价，历史会记住，是你，亲手”毁掉”了曾经充满希望的 OpenAI！</p> <p>&lt;!– [[2025-03-23]]</p>]]></content><author><name></name></author><category term="AI-Industry"/><category term="ai"/><category term="openai"/><category term="deepseek"/><category term="grok"/><category term="claude"/><summary type="html"><![CDATA[OpenAI 从开源理想主义到商业垄断，从技术创新到挤牙膏式发布，这家公司正在逐渐失去它的灵魂。本文将探讨 OpenAI 的衰落之路。]]></summary></entry><entry><title type="html">告别枯燥终端，迎接 Rich：让你的开发生活更舒心</title><link href="https://1587causalai.github.io/blog/2025/rich-terminal/" rel="alternate" type="text/html" title="告别枯燥终端，迎接 Rich：让你的开发生活更舒心"/><published>2025-02-19T00:00:00+08:00</published><updated>2025-02-19T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/rich-terminal</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/rich-terminal/"><![CDATA[<p>作为一名在代码世界里摸爬滚打了多年的老兵，我每天都要和终端打交道，查看各种各样的输出信息：编译日志、程序运行状态、数据处理结果…… 说实话，以前的终端输出给我的感觉就是——<strong>单调、乏味，甚至有些让人眼花缭乱</strong>。</p> <p>黑底白字，密密麻麻，长时间盯着屏幕，眼睛容易疲劳，信息也难以快速捕捉。 相信很多开发者朋友和我一样，都默默忍受着这种”不那么友好”的终端体验。</p> <p>直到我遇到了 <strong><code class="language-plaintext highlighter-rouge">rich</code></strong> 这个 Python 包，一切都改变了！ 它就像一股清流，瞬间滋润了我那干涸已久的终端世界，让我找回了久违的 <strong>“舒适感”</strong>。</p> <h2 id="为什么-rich-让我感到如此舒服">为什么 Rich 让我感到如此”舒服”？</h2> <p>这种”舒服感”并非玄学，而是 <code class="language-plaintext highlighter-rouge">rich</code> 包实实在在的功能所带来的。它让终端输出不再是枯燥的文本流，而是<strong>色彩丰富、层次分明、赏心悦目的艺术品</strong>。</p> <p>想象一下，当你查看程序输出时，关键信息用鲜艳的颜色突出显示，表格数据整齐排列，进度条动态展示任务进度，错误信息清晰醒目…… 这种<strong>一目了然、高效获取信息</strong>的感觉，难道不让人感到身心舒畅吗？</p> <p>更重要的是，<code class="language-plaintext highlighter-rouge">rich</code> 的美观并非花哨，而是<strong>实用至上</strong>。 它通过视觉上的优化，<strong>降低了信息噪音，提升了信息密度</strong>，让我能够更快地理解程序输出，更轻松地定位问题，从而<strong>提升开发效率，减少视觉疲劳</strong>。</p> <p>这种 “润物细无声” 的舒适感，才是 <code class="language-plaintext highlighter-rouge">rich</code> 最打动我的地方。</p> <h2 id="rich-的-舒适-特性大盘点">Rich 的 “舒适” 特性大盘点</h2> <p>那么，<code class="language-plaintext highlighter-rouge">rich</code> 究竟是如何做到让终端输出如此”舒服”的呢？ 下面就让我这个”老司机”带您一探究竟：</p> <p><img src="https://s2.loli.net/2025/02/19/UZxw52NJI4DGSL8.png" alt="20250219194243"/> <img src="https://s2.loli.net/2025/02/19/SOkuPTsmlLI5ngZ.png" alt="20250219194244"/></p> <h2 id="rich-的-舒适-用武之地">Rich 的 “舒适” 用武之地</h2> <p><code class="language-plaintext highlighter-rouge">rich</code> 的应用场景非常广泛，几乎所有需要终端输出的 Python 项目都可以从中受益。 作为一名资深开发者，我总结了以下几个 <code class="language-plaintext highlighter-rouge">rich</code> 的 “舒适” 应用场景：</p> <ul> <li><strong>命令行工具 (CLI 工具):</strong> 开发命令行工具时，<code class="language-plaintext highlighter-rouge">rich</code> 可以让你的工具输出更友好、更专业，提升用户体验。</li> <li><strong>脚本和自动化任务:</strong> 在自动化脚本中使用 <code class="language-plaintext highlighter-rouge">rich</code>，可以更清晰地展示脚本运行状态、日志信息，方便监控和调试。</li> <li><strong>日志输出:</strong> 使用 <code class="language-plaintext highlighter-rouge">rich</code> 格式化日志输出，让日志信息更易于阅读和分析，提高问题排查效率。</li> <li><strong>数据科学脚本:</strong> 在数据科学脚本中使用 <code class="language-plaintext highlighter-rouge">rich</code>，可以更美观地展示数据分析结果、图表、表格，方便数据探索和结果汇报。</li> <li><strong>长时间运行任务:</strong> 对于需要长时间运行的任务，<code class="language-plaintext highlighter-rouge">rich</code> 的进度条功能可以有效缓解用户的焦虑情绪，提升用户体验。</li> </ul> <h2 id="如何开始享受-rich-的-舒适-体验">如何开始享受 Rich 的 “舒适” 体验？</h2> <p>安装 <code class="language-plaintext highlighter-rouge">rich</code> 非常简单，只需一条命令：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>rich
</code></pre></div></div> <p>然后，在你的 Python 代码中导入 <code class="language-plaintext highlighter-rouge">rich</code> 包，就可以开始使用了。 最常用的入口点是 <code class="language-plaintext highlighter-rouge">rich.print</code> 函数，它用法和 Python 内置的 <code class="language-plaintext highlighter-rouge">print</code> 函数类似，但功能更强大。</p> <p>下面是一个简单的示例，展示如何使用 <code class="language-plaintext highlighter-rouge">rich</code> 输出彩色文本和表格：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">rich.console</span> <span class="kn">import</span> <span class="n">Console</span>
<span class="kn">from</span> <span class="n">rich.table</span> <span class="kn">import</span> <span class="n">Table</span>

<span class="n">console</span> <span class="o">=</span> <span class="nc">Console</span><span class="p">()</span>

<span class="c1"># 输出彩色文本
</span><span class="n">console</span><span class="p">.</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">[bold red]Hello[/bold red] [italic blue]World[/italic blue]!</span><span class="sh">"</span><span class="p">,</span> <span class="n">justify</span><span class="o">=</span><span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 创建表格
</span><span class="n">table</span> <span class="o">=</span> <span class="nc">Table</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">示例表格</span><span class="sh">"</span><span class="p">)</span>
<span class="n">table</span><span class="p">.</span><span class="nf">add_column</span><span class="p">(</span><span class="sh">"</span><span class="s">列 1</span><span class="sh">"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="sh">"</span><span class="s">cyan</span><span class="sh">"</span><span class="p">)</span>
<span class="n">table</span><span class="p">.</span><span class="nf">add_column</span><span class="p">(</span><span class="sh">"</span><span class="s">列 2</span><span class="sh">"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="sh">"</span><span class="s">magenta</span><span class="sh">"</span><span class="p">)</span>
<span class="n">table</span><span class="p">.</span><span class="nf">add_column</span><span class="p">(</span><span class="sh">"</span><span class="s">列 3</span><span class="sh">"</span><span class="p">,</span> <span class="n">justify</span><span class="o">=</span><span class="sh">"</span><span class="s">right</span><span class="sh">"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">)</span>
<span class="n">table</span><span class="p">.</span><span class="nf">add_row</span><span class="p">(</span><span class="sh">"</span><span class="s">数据 1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">数据 2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">数据 3</span><span class="sh">"</span><span class="p">)</span>
<span class="n">table</span><span class="p">.</span><span class="nf">add_row</span><span class="p">(</span><span class="sh">"</span><span class="s">数据 4</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">数据 5</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">数据 6</span><span class="sh">"</span><span class="p">)</span>
<span class="n">table</span><span class="p">.</span><span class="nf">add_row</span><span class="p">(</span><span class="sh">"</span><span class="s">数据 7</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">数据 8</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">数据 9</span><span class="sh">"</span><span class="p">)</span>

<span class="n">console</span><span class="p">.</span><span class="nf">print</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</code></pre></div></div> <p>运行这段代码，你就会在终端中看到带有彩色文本和表格的精美输出！</p> <h2 id="总结让-rich-成为你的开发-舒适区">总结：让 Rich 成为你的开发 “舒适区”</h2> <p>作为一名资深开发者，我强烈推荐大家尝试一下 <code class="language-plaintext highlighter-rouge">rich</code> 这个 Python 包。 它不仅仅是一个简单的 “美化终端” 工具，更是一种 <strong>提升开发体验、提高工作效率</strong> 的利器。</p> <p>告别枯燥乏味的终端输出，迎接 <code class="language-plaintext highlighter-rouge">rich</code> 带来的舒适与高效，让你的开发生活更加轻松愉快！ 相信我，一旦用上 <code class="language-plaintext highlighter-rouge">rich</code>，你就再也回不去了！ 😉</p> <p>赶紧行动起来，让 <code class="language-plaintext highlighter-rouge">rich</code> 成为你的开发 “舒适区” 吧！ 如果你在使用过程中有任何心得体会，欢迎在评论区分享，让我们一起交流学习，共同进步！</p> <p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQlavYeq0Sci6DK83kZXd0c5hQZ6AnLfdybTcrPG0uleK7DRD_KyCdguhXo70NE" alt="rich"/></p>]]></content><author><name></name></author><category term="tools"/><category term="python"/><category term="rich"/><category term="terminal-tools"/><category term="developer-experience"/><summary type="html"><![CDATA[作为一名开发者，终端是我们每天都要打交道的工具。本文介绍了如何使用 Rich 这个 Python 包来改善终端输出体验，让开发工作更加舒适高效。]]></summary></entry><entry><title type="html">如何高效使用 DeepSeek-R1? 一些提示词工程本质思考</title><link href="https://1587causalai.github.io/blog/2025/deepseek-prompt/" rel="alternate" type="text/html" title="如何高效使用 DeepSeek-R1? 一些提示词工程本质思考"/><published>2025-01-29T00:00:00+08:00</published><updated>2025-01-29T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/deepseek-prompt</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/deepseek-prompt/"><![CDATA[<p>在当今人工智能领域，一场来自东方的开源风暴正席卷全球，DeepSeek-R1 模型的横空出世，犹如破晓之光，照亮了 AI 发展的新路径。中国创业公司深度求索凭借这一创新力作，不仅在技术性能上与顶尖商业模型并驾齐驱，更以其显著的成本优势，引发各界广泛关注，从美国硅谷的科技巨头到华尔街的投资精英，目光纷纷聚焦于此，其应用程序在全球各国 App Store 免费榜单上的迅猛攀升，更是彰显了其强大的市场吸引力。</p> <h2 id="一deepseek-r1技术突破与全球反响">一、DeepSeek-R1：技术突破与全球反响</h2> <p>DeepSeek-R1 模型在多个关键领域展现出卓越性能，数学运算的精准、编程逻辑的严谨以及自然语言推理的流畅，都使其成为行业焦点。尤为值得一提的是，其成本仅为 OpenAI o1 模型的三十分之一，这一成本的大幅降低，无疑为高性能 AI 模型的广泛应用扫清了障碍，让更多企业和开发者能够触及前沿 AI 技术，开启创新之旅。</p> <p>其独特的训练方法更是引发热议，完全依赖强化学习（RL），摒弃了传统的监督微调（SFT），这一创新之举赋予了模型更强的泛化能力。在训练过程中，模型通过不断试错、自我优化，如同人类学习成长般，逐步提升推理能力，而非受限于预先定义的固定任务和格式套路，从而在多个基准测试中脱颖而出，甚至在某些领域超越了 OpenAI 的产品，为 AI 技术发展提供了全新思路。 这一突破性进展在全球范围内激起千层浪。美国前总统特朗普将其视为美国科技行业的 “警钟”，呼吁加大竞争力度，以应对来自东方的强劲挑战。OpenAI 首席执行官萨姆・阿尔特曼虽表示赞赏，但也深知竞争压力，承诺加快新模型发布步伐。而特斯拉 CEO 埃隆・马斯克则对 DeepSeek 的成本声称持怀疑态度，暗示背后可能存在更多高端芯片的隐秘投入。受此影响，技术股市场波动剧烈，纳斯达克指数在发布当天下跌 3.1%，英伟达股价暴跌近 17%，投资者开始重新审视美国公司在 AI 数据中心和基础设施领域的投资布局，美国科技巨头的市场主导地位面临前所未有的挑战。</p> <h2 id="二提示词工程的正本清源">二、提示词工程的正本清源</h2> <p>在这场技术变革浪潮中，作为实际使用者，我们最为关切的无疑是如何真正驾驭 DeepSeek-R1 这一强大模型，让其为我们的工作、学习和生活赋能。在传统大模型应用中，用户常常陷入 “咒语设计” 的困境，<strong>精心雕琢思维链（CoT）、角色扮演（Role-Play）、格式模板等提示技巧</strong>，试图通过复杂繁琐的指令来获取理想答案。然而，DeepSeek-R1 的出现打破了这一固有模式，它宛如一位更懂用户的知心伙伴，无需繁复修饰，只需我们直接提问，便能给出精准回应。</p> <p>有趣的是，广为流传的一个提示词是 “说人话”，这看似简单的话语，实则道出了 DeepSeek-R1 与众不同的特质。它更擅长与人类进行自然流畅的沟通，摒弃了以往复杂提示词如同魔法咒语般的晦涩难懂，真正回归到人机协作的本质。而这一切，得益于其构建时的创新理念，即原始地采用强化学习（RL）而非监督微调（SFT）方法。</p> <p>在主流大模型构建中，监督微调（SFT）被视为常规路径，通过预先定义大量任务和格式套路，让模型去学习模仿。但 DeepSeek-R1 另辟蹊径，坚信大模型应仿效人类学习方式，借助强化学习，在不断试错中优化自身，从而具备更强的泛化能力，不被固定任务和格式所束缚。这一理念的转变，为我们揭示了<strong>使用 AI 的 “第一性原理” —— 说清楚需求，定义好问题，提供好上下文:</strong></p> <ul> <li>你的需求是什么? 合适的定义任务和问题是和 AI 协作的关键第一步, 当需求很复杂的时候, 我们需要进行适当的拆解.</li> <li>问题背景是什么? 各种问题背景信息, 用户经验和偏好信息等一起提供给模型, 模型会基于这些信息给出更好的答案.</li> </ul> <p>当我们与 DeepSeek-R1 交互时，无需再费心琢磨那些花哨的提示技巧，而是将精力聚焦于明确自身需求。例如，在寻求写作灵感时，直接告知模型 “我需要一篇关于环保主题的文章大纲，要求涵盖当前环境问题、解决方案以及未来展望，字数在 1000 字左右”，如此清晰明确的需求描述，远胜于复杂的思维链引导。同时，定义好问题至关重要，若你是一名程序员，遇到代码报错，精准地向模型阐述 “我在使用 Python 编写爬虫程序时，遇到了 ‘requests’ 库连接超时的问题，已尝试过检查网络连接和服务器状态，但仍未解决，该如何排查？” 这样具体且聚焦的问题，能让模型迅速定位关键点，给出有效建议。此外，提供好上下文也不可或缺，若你正在进行学术研究，向模型展示相关文献资料、研究背景等上下文信息，它便能基于此为你拓展思路、补充论据。</p> <h2 id="三总结">三、总结</h2> <p>DeepSeek-R1 的诞生，无疑是人工智能领域的一个里程碑, 标志着开源AI模型的崛起。对于广大用户而言，掌握其使用方法的关键，在于深刻领悟提示词工程的本质 —— 说清楚需求，定义好问题，提供好上下文。当我们摒弃形式主义的复杂技巧，回归到与 AI 简单直接、清晰准确的沟通时，DeepSeek-R1 将成为我们最得力的助手，助力我们在 AI 时代开启高效智能的协作之旅，解锁无限可能。</p> ]]></content><author><name></name></author><category term="prompt"/><category term="DeepSeek-R1"/><category term="Prompt-Engineering"/><category term="LLM"/><category term="AI"/><summary type="html"><![CDATA[当普通大模型用户沉迷于魔法咒语时，真正的高手早已返璞归真。那些复杂的思维链模板、角色扮演话术，往往让需求迷失在形式主义中。与AI协作的道，在于说清楚需求, 定义好问题, 提供好上下文.]]></summary></entry><entry><title type="html">LlamaFactory 使用教程：轻松实现大模型微调</title><link href="https://1587causalai.github.io/blog/2025/llama-factory-tutorial/" rel="alternate" type="text/html" title="LlamaFactory 使用教程：轻松实现大模型微调"/><published>2025-01-04T00:00:00+08:00</published><updated>2025-01-04T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/llama-factory-tutorial</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/llama-factory-tutorial/"><![CDATA[<p>各位朋友，今天我必须、一定要、强烈地安利一个神奇的工具——<strong>LlamaFactory</strong>！如果你和我一样，对微调大型语言模型充满热情，但又被各种复杂的命令行参数和环境配置搞得头大，那么请你务必认真看完这篇文章。因为，它真的会让你尖叫！</p> <p>我这篇文章将理论结合实践来写，通过两个具体步骤来展示 LlamaFactory 的强大：</p> <ol> <li>最快速地跑一个最简单的 DPO，体现它的易用性</li> <li>使用本地模型和本地数据进行 DPO，体现它的定制性</li> </ol> <h2 id="初见-llamafactory惊艳的第一印象">初见 LlamaFactory：惊艳的第一印象</h2> <p>我之前也尝试过一些微调工具，要么就是配置起来像解一道高数题，要么就是界面简陋得让人想砸键盘。但是，LlamaFactory-CLI WebUI 的出现，简直就像一道耀眼的光芒，瞬间照亮了我略显昏暗的 AI 炼丹之路！</p> <p>让我们从最简单的安装开始：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone <span class="nt">--depth</span> 1 https://github.com/hiyouga/LLaMA-Factory.git
<span class="nb">cd </span>LLaMA-Factory
pip <span class="nb">install</span> <span class="nt">-e</span> <span class="s2">".[torch]"</span>
</code></pre></div></div> <p>安装完成后，只需一行命令就能启动这个神奇的界面：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llamafactory-cli webui
</code></pre></div></div> <p><strong>打开浏览器，映入眼帘的界面让我惊呆了！</strong></p> <p>简洁明了的布局瞬间让人心情舒畅，各种参数选项排列有序，色彩搭配也恰到好处，完全没有那种”工程师风格”的冰冷感。这绝对是我见过最漂亮的 CLI WebUI 之一，用起来赏心悦目，效率都感觉提升了不少！</p> <h2 id="dpo-训练实战从理论到实践的完美演绎">DPO 训练实战：从理论到实践的完美演绎</h2> <p>别看界面漂亮，功能可一点都不含糊！我马上开始了一次实战训练，选择了最简单但也最能体现特点的配置：</p> <ul> <li>模型：<code class="language-plaintext highlighter-rouge">Qwen/Qwen-1_8B</code>（轻量级但效果不错的中文模型）</li> <li>数据集：<code class="language-plaintext highlighter-rouge">hh_rlhf_en</code>（内置的人类反馈数据集）</li> <li>训练方法：DPO（Direct Preference Optimization）</li> </ul> <p>点击”开始训练”后，眼前的画面让我感动得想哭：</p> <div style="text-align: center;"> <img src="https://s2.loli.net/2025/01/04/mu69nbXC7gkjowR.png" alt="训练界面" style="max-width: 85%; height: auto;"/> </div> <p><strong>这个训练过程简直就是一场视觉盛宴！</strong></p> <ul> <li>Loss 曲线平滑下降，就像一个优雅的舞者</li> <li>学习率的调整精准而富有节奏</li> <li>训练进度条稳步前进，让人心里踏实</li> <li>GPU 显存使用情况一目了然，再也不用担心爆显存</li> </ul> <p>更让我觉得贴心的是，WebUI 还贴心地生成了完整的训练命令：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llamafactory-cli train <span class="se">\</span>
    <span class="nt">--stage</span> dpo <span class="se">\</span>
    <span class="nt">--do_train</span> True <span class="se">\</span>
    <span class="nt">--model_name_or_path</span> Qwen/Qwen-1_8B <span class="se">\</span>
    <span class="nt">--preprocessing_num_workers</span> 16 <span class="se">\</span>
    <span class="nt">--finetuning_type</span> lora <span class="se">\</span>
    <span class="nt">--template</span> default <span class="se">\</span>
    <span class="nt">--flash_attn</span> auto <span class="se">\</span>
    <span class="nt">--dataset_dir</span> data <span class="se">\</span>
    <span class="nt">--dataset</span> hh_rlhf_en <span class="se">\</span>
    <span class="nt">--cutoff_len</span> 2048 <span class="se">\</span>
    <span class="nt">--learning_rate</span> 5e-05 <span class="se">\</span>
    <span class="nt">--num_train_epochs</span> 1.0 <span class="se">\</span>
    <span class="nt">--max_samples</span> 1000 <span class="se">\</span>
    <span class="nt">--per_device_train_batch_size</span> 2 <span class="se">\</span>
    <span class="nt">--gradient_accumulation_steps</span> 4 <span class="se">\</span>
    <span class="nt">--lr_scheduler_type</span> cosine <span class="se">\</span>
    <span class="nt">--max_grad_norm</span> 1.0 <span class="se">\</span>
    <span class="nt">--logging_steps</span> 5 <span class="se">\</span>
    <span class="nt">--save_steps</span> 100 <span class="se">\</span>
    <span class="nt">--warmup_steps</span> 0 <span class="se">\</span>
    <span class="nt">--packing</span> False <span class="se">\</span>
    <span class="nt">--report_to</span> none <span class="se">\</span>
    <span class="nt">--output_dir</span> saves/Qwen-1.8B/lora/train_2025-01-04-12-04-07 <span class="se">\</span>
    <span class="nt">--bf16</span> True <span class="se">\</span>
    <span class="nt">--plot_loss</span> True <span class="se">\</span>
    <span class="nt">--trust_remote_code</span> True <span class="se">\</span>
    <span class="nt">--ddp_timeout</span> 180000000 <span class="se">\</span>
    <span class="nt">--include_num_input_tokens_seen</span> True <span class="se">\</span>
    <span class="nt">--optim</span> adamw_torch <span class="se">\</span>
    <span class="nt">--lora_rank</span> 8 <span class="se">\</span>
    <span class="nt">--lora_alpha</span> 16 <span class="se">\</span>
    <span class="nt">--lora_dropout</span> 0 <span class="se">\</span>
    <span class="nt">--lora_target</span> all <span class="se">\</span>
    <span class="nt">--pref_beta</span> 0.1 <span class="se">\</span>
    <span class="nt">--pref_ftx</span> 0 <span class="se">\</span>
    <span class="nt">--pref_loss</span> sigmoid
</code></pre></div></div> <p>这个命令不仅仅是一串参数的组合，它展示了 LlamaFactory 对微调过程的深刻理解：使用 LoRA 降低显存占用，采用较小的学习率避免破坏预训练知识，设置合理的 batch size 和梯度累积步数…每个参数都经过精心调优。</p> <h2 id="定制化训练从示例到本地模型">定制化训练：从示例到本地模型</h2> <p>这个预览命令功能真的是太贴心了！有了它，我们就可以轻松地将这个训练过程迁移到自己的本地模型上。我用的是本地的 InternLM2-1.8B 模型，只需要将命令中的模型路径改为本地路径：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llamafactory-cli train <span class="se">\</span>
    <span class="nt">--stage</span> dpo <span class="se">\</span>
    <span class="nt">--do_train</span> True <span class="se">\</span>
    <span class="nt">--model_name_or_path</span> /path/to/internlm2-1.8b <span class="se">\</span>
    <span class="nt">--preprocessing_num_workers</span> 16 <span class="se">\</span>
    <span class="nt">--finetuning_type</span> lora <span class="se">\</span>
    <span class="nt">--template</span> default <span class="se">\</span>
    <span class="nt">--flash_attn</span> auto <span class="se">\</span>
    <span class="nt">--dataset_dir</span> data <span class="se">\</span>
    <span class="nt">--dataset</span> hh_rlhf_en <span class="se">\</span>
    <span class="nt">--cutoff_len</span> 2048 <span class="se">\</span>
    <span class="nt">--learning_rate</span> 5e-05 <span class="se">\</span>
    <span class="nt">--num_train_epochs</span> 1.0 <span class="se">\</span>
    <span class="nt">--max_samples</span> 1000 <span class="se">\</span>
    <span class="nt">--per_device_train_batch_size</span> 2 <span class="se">\</span>
    <span class="nt">--gradient_accumulation_steps</span> 4 <span class="se">\</span>
    <span class="nt">--lr_scheduler_type</span> cosine <span class="se">\</span>
    <span class="nt">--max_grad_norm</span> 1.0 <span class="se">\</span>
    <span class="nt">--logging_steps</span> 5 <span class="se">\</span>
    <span class="nt">--save_steps</span> 100 <span class="se">\</span>
    <span class="nt">--warmup_steps</span> 0 <span class="se">\</span>
    <span class="nt">--packing</span> False <span class="se">\</span>
    <span class="nt">--report_to</span> none <span class="se">\</span>
    <span class="nt">--output_dir</span> saves/Qwen-1.8B/lora/train_2025-01-04-12-04-07 <span class="se">\</span>
    <span class="nt">--bf16</span> True <span class="se">\</span>
    <span class="nt">--plot_loss</span> True <span class="se">\</span>
    <span class="nt">--trust_remote_code</span> True <span class="se">\</span>
    <span class="nt">--ddp_timeout</span> 180000000 <span class="se">\</span>
    <span class="nt">--include_num_input_tokens_seen</span> True <span class="se">\</span>
    <span class="nt">--optim</span> adamw_torch <span class="se">\</span>
    <span class="nt">--lora_rank</span> 8 <span class="se">\</span>
    <span class="nt">--lora_alpha</span> 16 <span class="se">\</span>
    <span class="nt">--lora_dropout</span> 0 <span class="se">\</span>
    <span class="nt">--lora_target</span> all <span class="se">\</span>
    <span class="nt">--pref_beta</span> 0.1 <span class="se">\</span>
    <span class="nt">--pref_ftx</span> 0 <span class="se">\</span>
    <span class="nt">--pref_loss</span> sigmoid
</code></pre></div></div> <p>训练完成后，在 <code class="language-plaintext highlighter-rouge">saves/internlm2-1.8b/lora/dpo_train/</code> 目录下生成了训练结果：</p> <ul> <li><code class="language-plaintext highlighter-rouge">training_loss.png</code>：训练损失曲线</li> <li><code class="language-plaintext highlighter-rouge">training_rewards_accuracies.png</code>：奖励和准确率曲线</li> <li><code class="language-plaintext highlighter-rouge">adapter_model.safetensors</code>：训练得到的 LoRA 权重</li> <li>其他配置文件和中间检查点</li> </ul> <p>训练损失曲线:</p> <div style="text-align: center;"> <img src="https://s2.loli.net/2025/01/04/XDZa4zP8sL6pH7f.png" alt="训练损失曲线" style="max-width: 85%; height: auto;"/> </div> <p>LlamaFactory 确实是一个非常好用的工具，它让模型微调变得如此简单。通过实践我发现，它的 WebUI 不仅让操作变得直观，更重要的是帮助我们理解了微调过程中的各个参数和步骤。不过需要注意的是，在实际使用中，数据质量和模型选择仍然是最关键的因素。工具再好，也要有优质的数据和合适的基座模型才能训练出好的效果。更多解读理解文档请参考 <a href="https://1587causalai.github.io/llama_factory/">link</a>。</p>]]></content><author><name></name></author><category term="ml-engineering"/><category term="llm"/><category term="fine-tuning"/><summary type="html"><![CDATA[本文介绍如何使用 LlamaFactory 进行大模型微调，包括使用 WebUI 和命令行两种方式]]></summary></entry><entry><title type="html">从个体偏见到群体智慧：人类认知的算力限制与演化</title><link href="https://1587causalai.github.io/blog/2024/human-bias-and-interview/" rel="alternate" type="text/html" title="从个体偏见到群体智慧：人类认知的算力限制与演化"/><published>2024-12-25T00:00:00+08:00</published><updated>2024-12-25T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/human-bias-and-interview</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/human-bias-and-interview/"><![CDATA[<h1 id="人类偏见统一理论与面试感悟">人类偏见统一理论与面试感悟</h1> <h2 id="人类偏见的统一理论">人类偏见的统一理论</h2> <p>在心理学领域，我们经常听到各种各样的认知偏误：确认偏误、锚定效应、损失厌恶、过度自信偏差、聚光灯效应等等。这些偏误就像成语典故一样，已经成为现代人必须了解的思维模型。但是，它们之间是否存在某种内在的联系？</p> <p>最近，两位德国心理学家艾琳·厄伯斯特（Aileen Oeberst）和罗兰·因霍夫（Roland Imhoff）提出了一个令人耳目一新的统一理论：</p> <p><strong>任何偏见 = 一个信念 + 确认偏误</strong></p> <p>这个简单而优雅的公式背后，隐藏着人类思维的基本模式。让我们深入理解这个理论：</p> <h3 id="六个根本信念">六个根本信念</h3> <ol> <li> <p><strong>“我的经验是合理的参考”</strong></p> <ul> <li>聚光灯效应：我们总觉得别人在关注自己</li> <li>透明度错觉：认为自己的想法对他人来说显而易见</li> <li>虚假共识效应：默认自己的信念是大众共识</li> </ul> </li> <li> <p><strong>“我对世界的判断是正确的”</strong></p> <ul> <li>敌意媒体偏误：认为不利于自己的中立报道是故意抹黑</li> <li>偏误盲点：容易发现他人的偏误，却看不到自己的</li> </ul> </li> <li> <p><strong>“我是好的”</strong></p> <ul> <li>优于平均水平效应：认为自己各方面能力都高于平均</li> <li>自利归因偏差：成功归因于自己，失败归因于外部</li> </ul> </li> <li> <p><strong>“我的群体是合理的参考”</strong></p> <ul> <li>种族中心主义：以自己群体的标准评判其他群体</li> <li>群体内投射：认为自己群体更能代表普遍性</li> </ul> </li> <li> <p><strong>“我们群体是好的”</strong></p> <ul> <li>内群体偏好：对自己群体的无条件认同</li> <li>外群体贬低：对其他群体的本能排斥</li> </ul> </li> <li> <p><strong>“人们的属性决定结果”</strong></p> <ul> <li>基本归因谬误：过分强调个人特质，忽视环境因素</li> </ul> </li> </ol> <p>理解了这些根本信念后，一个自然的问题是：为什么人类会形成这些系统性的偏见？这就需要我们从更基础的角度 - 计算复杂性的视角来思考。</p> <h3 id="偏见的本质算力限制与群体智慧">偏见的本质：算力限制与群体智慧</h3> <p>从计算复杂性的角度来看，Stephen Wolfram提出了一个发人深省的观点：人类作为一个智能系统，面临着根本的算力限制。就像热力学第二定律、相对论、量子力学这些物理定律可能是我们认知局限的产物一样，个体的心理偏误也可能源于我们处理信息能力的限制。</p> <p>面对复杂系统，每个个体都不得不使用简化模型来理解世界。这些简化模型就表现为各种认知偏误。但有趣的是，正是这些看似”有缺陷”的个体，在组成群体后反而展现出了更强的泛化能力。</p> <h3 id="从随机实验到群体智慧一个演化视角">从随机实验到群体智慧：一个演化视角</h3> <p>群体智慧的形成过程可以从三个层次来理解：</p> <ol> <li> <p><strong>个体层面的简化与偏见</strong></p> <ul> <li>每个人的偏见都是对世界的一种简化建模</li> <li>这种简化源于算力和信息处理能力的限制</li> <li>不同个体会形成不同的简化模型</li> </ul> </li> <li> <p><strong>群体层面的随机实验</strong></p> <ul> <li>每个个体的偏见都是一次”随机实验”</li> <li>不同的偏见产生不同的决策结果</li> <li>群体通过这些实验积累经验</li> <li>某些偏见会被证明更有适应性</li> </ul> </li> <li> <p><strong>演化层面的智慧涌现</strong></p> <ul> <li>能够识别并克服特定偏见的个体获得优势</li> <li>这种优势推动了群体的整体进化</li> <li>群体在这个过程中建立更准确的因果模型</li> <li>“偏见多样性”提供了系统的鲁棒性</li> </ul> </li> </ol> <p>这个过程揭示了一个反直觉的真理：</p> <ul> <li>极致效率的系统往往是脆弱的</li> <li>看似”低效”的偏见多样性反而提供了适应能力</li> <li>个体通过意识到偏见，既促进自身发展，也推动群体进化</li> </ul> <p>这些根本信念本质上都指向一个核心：<strong>“我跟别人很不一样，对此我感到骄傲”</strong>。这种”我执”不仅仅是认知上的偏差，更是人类社会发展的重要动力。从计算的角度看，这种差异性恰恰是群体智慧得以形成的基础。</p> <h2 id="面试场景中的偏见一些个人感悟">面试场景中的偏见：一些个人感悟</h2> <p>面试本质上是一个排序问题, 有一些基本的规则。在经历了多次面试后，我对面试场景中这些规律有了更深的体会。</p> <h3 id="算法题考察的两难">算法题考察的两难</h3> <p>最近的面试经历让我深深体会到这个矛盾：虽然在这个人工智能时代，用算法题去考察一个候选人的能力似乎已经不太合适了，但是还有比它更好的普适性考察方式吗？</p> <p>这个矛盾背后其实反映了更深层的问题：</p> <ul> <li>面试官需要一个相对客观的排序标准</li> <li>在有限时间内，算法题仍是最”公平”的量化指标</li> <li>这本身就反映了人类对”可量化标准”的偏好</li> </ul> <p>面试官的决策困境. 在面试中，我注意到一个有趣的现象：即使某个面试官看到了你独特的优势，他也未必能超越常规的评判标准给出录取决定。因为：</p> <ul> <li>作出非同寻常的判断，需要非同寻常的证据</li> <li>面试官需要承担更大的决策风险</li> <li>这反映了”损失厌恶”这一典型偏见</li> </ul> <h3 id="关于空白期的对话">关于空白期的对话</h3> <p>在东方财富的面试中，我遇到了一个看似棘手但实际是送分的问题：”你为什么gap这么久？”</p> <p>我的回答应该是：</p> <ul> <li>“我下定决心想要转向大模型方向”</li> <li>“因为背景不太匹配，所以需要花很多时间从零开始”</li> <li>“我系统化地训练了相关技能”</li> <li>“实际落地了大模型相关项目”</li> </ul> <p>我这个送分题没有回答好, 算是惨痛教训吧. 这个经历让我明白尊重面试场景的重要性：</p> <ul> <li>强调主动学习和成长</li> <li>展示明确的职业规划</li> <li>用具体项目证明能力</li> </ul> <h2 id="结语">结语</h2> <p>理解人类偏见的统一理论，不仅帮助我们认识自己的思维局限，也让我们更好地理解和尊重面试场景的特殊性。在AI时代，这些”偏见”恰恰彰显了人性的独特之处。</p> <p>正如格雷戈里·贝特森所说的”分裂演化”现象，人类社会的多样性正是建立在这些偏见之上。这种多样性虽然可能导致一些效率损失，但却是创新和进步的源泉。</p> <p>在面对偏见时，我们需要：</p> <ol> <li>认识到偏见的普遍性和必然性</li> <li>理解偏见背后的深层原因</li> <li>在保持个性的同时，尊重场景规则</li> <li>用建设性的方式应对偏见带来的挑战</li> </ol> <p>最终，无论是克服偏见还是准备面试，关键都在于保持开放和谦逊的心态。认识到自己的局限，尊重场景的规则，这样才能获得真正的进步。</p>]]></content><author><name></name></author><category term="psychology"/><category term="interview-experience"/><category term="reflection"/><summary type="html"><![CDATA[关于人类偏见的统一理论思考以及面试场景的规则]]></summary></entry><entry><title type="html">AI-Powered 项目管理：学术严谨性 + 知识普及性 + 代码快速实现</title><link href="https://1587causalai.github.io/blog/2024/project-workflow/" rel="alternate" type="text/html" title="AI-Powered 项目管理：学术严谨性 + 知识普及性 + 代码快速实现"/><published>2024-12-23T18:00:00+08:00</published><updated>2024-12-23T18:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/project-workflow</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/project-workflow/"><![CDATA[<p>上个月我写了一篇关于如何用 Overleaf 和 Cursor 来提升论文写作效率的文章。这个月的实践让我对整个研究工作流有了更深层的思考 —— 这不仅仅是工具的选择，而是关于如何更好地创造和传播知识。</p> <h2 id="为什么需要这样的工作方式">为什么需要这样的工作方式？</h2> <p>在当今这个信息爆炸的时代，我们面临着三个核心挑战：</p> <ul> <li>如何保证研究的专业性和可信度</li> <li>如何让复杂的研究更容易被理解和应用</li> <li>如何在保证质量的同时提高工作效率</li> </ul> <p>经过反复实践，我找到了一个可能的答案：将人类最严谨的知识创造方式（学术研究）、最有效的知识传播方式（通俗化表达）和最先进的效率工具（AI辅助）结合起来。 一个案例:</p> <h2 id="这个工作流程的三个支柱">这个工作流程的三个支柱</h2> <h3 id="第一支柱学术论文的严谨性">第一支柱：学术论文的严谨性</h3> <p>学术论文是人类历史上最可信的表达观点的方式。通过严格的同行评议、完整的推导论证和规范的引用体系，它确保了知识的可靠性和可追溯性。在我的工作流程中，这体现为：</p> <ul> <li>用 Overleaf（或其他 LaTeX 编辑器）来写作论文</li> <li>遵循学术规范，确保每个观点都有充分的论证</li> <li>通过版本控制保存思维的演进过程</li> </ul> <h3 id="第二支柱博客的普及性">第二支柱：博客的普及性</h3> <p>博客（或其他形式的通俗文档）是连接专业研究和普通读者的桥梁。通过 Docsify、ReadTheDocs 等工具，我们可以：</p> <ul> <li>用通俗易懂的语言解释复杂的概念</li> <li>记录研究过程中的思考和灵感</li> <li>让研究成果能被更广泛的受众理解和应用</li> </ul> <h3 id="第三支柱ai-的效率提升">第三支柱：AI 的效率提升</h3> <p>AI 工具（如 Cursor、WindSurf 等）代表了现代科技对效率的追求。它们帮助我们：</p> <ul> <li>快速实现想法，验证假设</li> <li>自动化重复性工作</li> <li>在不牺牲质量的前提下加速整个研究过程</li> </ul> <h2 id="如何实现这个工作流一个案例">如何实现这个工作流？一个案例</h2> <p>具体实施时，我选择以一个统一的项目仓库为核心：</p> <ol> <li>首先在 Overleaf 创建项目（可以与 GitHub 同步）</li> <li>在项目中建立清晰的结构： <ul> <li><code class="language-plaintext highlighter-rouge">paper/</code>: 严谨的学术论文</li> <li><code class="language-plaintext highlighter-rouge">docs/</code>: 通俗化的项目文档（可以用任何适合的文档工具部署）</li> <li><code class="language-plaintext highlighter-rouge">src/</code>: 相关的代码实现</li> </ul> </li> <li>使用现代 AI 工具辅助整个过程</li> </ol> <p>日常工作流程：</p> <ul> <li>在论文中严谨地论证核心观点</li> <li>在文档中用通俗语言分享想法和进展</li> <li>通过代码将想法落地为实际应用</li> <li>让 AI 工具贯穿始终，提供智能辅助</li> </ul> <p>用研究项目的严谨框架来驱动一个可能演变的学习/科普项目, 一个符合这个理念的结构：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.
├── paper/                      # 核心研究论文相关
│   ├── main.tex               # 论文主体
│   ├── figures/               # 论文图表
│   └── data/                  # 实验数据
│
├── docs/                       # 文档网站（精简核心）
│   ├── overview/              # 项目概览
│   │   └── introduction.md    # 项目介绍
│   ├── progress/              # 项目进展
│   │   └── research-log.md    # 研究日志
│   ├── tutorials/             # 教程（可扩展）
│   └── references/            # 参考资料
│
├── research/                   # 研究过程文档 (内部使用)
│   ├── literature/            # 文献阅读笔记
│   ├── methodology/           # 研究方法设计
│   └── analysis/             # 数据分析记录
│
└── src/                       # 代码实现
    ├── core/                  # 核心实验代码
    ├── analysis/             # 数据分析代码
    └── examples/             # 示例代码（可扩展为教程代码）
</code></pre></div></div> <p>这个设计的特点：</p> <ol> <li> <p><strong>研究驱动</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">paper/</code> 作为项目的核心，确保学术严谨性</li> <li><code class="language-plaintext highlighter-rouge">research/</code> 存放详细的研究过程文档，便于追踪思路</li> </ul> </li> <li> <p><strong>灵活演变</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">docs/</code> 采用模块化结构，可以根据项目性质逐步扩展</li> <li>从研究发现到教程的自然过渡</li> <li>代码结构支持从实验到教学的转换</li> </ul> </li> <li><strong>双层文档</strong> <ul> <li><code class="language-plaintext highlighter-rouge">docs/</code>: 精炼的对外窗口，展示核心内容</li> <li><code class="language-plaintext highlighter-rouge">research/</code>: 详细的内部文档，记录完整研究过程</li> </ul> </li> <li><strong>渐进式扩展</strong> <ul> <li>初期专注于研究部分</li> <li>中期可以增加实验结果分析</li> <li>后期可以扩展为教程和科普内容</li> </ul> </li> </ol> <p>这个结构的优势是：</p> <ol> <li>保持了研究项目的严谨框架</li> <li>提供了清晰的对外展示窗口</li> <li>支持项目性质的自然演变</li> <li>内外文档分离，既保证了完整性又保持了对外简洁</li> </ol> <h2 id="这种方式的深远意义">这种方式的深远意义</h2> <p>这个工作流程的精髓在于它完美平衡了三个关键要素：</p> <ul> <li>学术论文确保了知识的可靠性</li> <li>通俗文档保证了知识的可及性</li> <li>AI 工具带来了前所未有的效率</li> </ul> <p>这不仅仅是一个工具的选择问题，而是关于如何在人工智能时代更好地创造和传播知识的思考。它让我们能够既保持学术的严谨，又不失传播的普及性，同时还能享受现代技术带来的效率提升。</p> <h2 id="写在最后">写在最后</h2> <p>这种工作方式代表了我对未来研究工作的思考：如何让专业研究既严谨又亲民，如何让知识创造既高效又可靠。它可能不是完美的答案，但却是一个值得尝试的方向。</p> <p>如果你也在思考如何更好地进行研究工作，欢迎分享你的想法！</p>]]></content><author><name></name></author><category term="tools"/><category term="research"/><category term="workflow"/><category term="AI"/><category term="knowledge-sharing"/><summary type="html"><![CDATA[一个融合学术论文、项目文档和代码实现的 AI-Powered 现代化知识创造与传播体系]]></summary></entry><entry><title type="html">大模型算法工程师面试经验与反思</title><link href="https://1587causalai.github.io/blog/2024/interview-experience/" rel="alternate" type="text/html" title="大模型算法工程师面试经验与反思"/><published>2024-12-10T00:00:00+08:00</published><updated>2024-12-10T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/interview-experience</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/interview-experience/"><![CDATA[<h1 id="大模型算法工程师面试经验与反思">大模型算法工程师面试经验与反思</h1> <h2 id="引言">引言</h2> <p>作为一名算法工程师，面试是职业生涯中不可避免的过程。本文将结合我最近在上海人工智能实验室评测大模型算法工程师岗位的面试经历，分享面试准备的要点和个人的深刻教训。</p> <h2 id="1-面试准备的关键点">1. 面试准备的关键点</h2> <h3 id="11-简历准备">1.1 简历准备</h3> <ul> <li><strong>项目经验描述</strong> <ul> <li>不要平铺直述，要用 STAR 法则（情境、任务、行动、结果）</li> <li>突出个人贡献和解决问题的能力</li> <li>用数据说话，比如：”优化后性能提升了 30%”</li> </ul> </li> </ul> <blockquote> <p><strong>个人反思</strong>：这次面试中，我在项目描述环节表现非常不理想。讲项目的时候逻辑非常混乱，没有应用STAR法则。如果按照STAR法则来组织语言，应该是这样的：</p> <ul> <li><strong>情境</strong>：介绍项目背景和面临的挑战</li> <li><strong>任务</strong>：明确说明我的职责</li> <li><strong>行动</strong>：详细描述采取的技术方案</li> <li><strong>结果</strong>：用数据说明项目成果</li> </ul> </blockquote> <ul> <li><strong>技术栈展示</strong> <ul> <li>分层次列举：熟练掌握的放在前面</li> <li>不要列举所有接触过的技术</li> <li>确保写在简历上的都能讲清楚原理</li> </ul> </li> </ul> <blockquote> <p><strong>个人反思</strong>：面试官对RLHF问了很多深入的问题，但我的回答支支吾吾。这暴露出一个问题：简历上写的技术一定要准备充分，必须能讲清楚具体原理和实现细节。</p> </blockquote> <h3 id="12-技术准备">1.2 技术准备</h3> <ul> <li><strong>必备知识点</strong> <ul> <li>计算机基础：操作系统、计算机网络、数据结构</li> <li>深度学习基础：模型结构、训练技巧、优化方法</li> <li>大模型相关：预训练、微调、RLHF</li> </ul> </li> <li><strong>算法准备</strong> <ul> <li>LeetCode 经典题目</li> <li>常见数据结构的实现</li> <li>面试高频算法题型</li> </ul> </li> </ul> <blockquote> <p><strong>个人反思</strong>：面试中遇到一道简单的链表题目，但当时大脑一片空白，没能很好地解决。这说明即使是最基础的算法题也不能掉以轻心，平时要多加练习，保持手感。</p> </blockquote> <h2 id="2-面试现场的注意事项">2. 面试现场的注意事项</h2> <h3 id="21-技术面试中的坑">2.1 技术面试中的坑</h3> <ul> <li> <p><strong>项目经验描述不够深入</strong></p> <ul> <li>❌ 错误示范：”我负责了模型训练和优化”</li> <li>✅ 正确示范：”我设计并实现了基于RLHF的对话模型优化方案，通过改进奖励模型和采样策略，将模型在人类偏好评估上的得分提升了40%”</li> </ul> </li> <li> <p><strong>技术问题回答不当</strong></p> <ul> <li>❌ 错误示范：不懂装懂，答非所问</li> <li>✅ 正确示范：诚实承认不了解，但表达学习意愿和解决问题的思路</li> </ul> </li> </ul> <h3 id="22-沟通技巧">2.2 沟通技巧</h3> <ul> <li>保持冷静</li> <li>注意表达的逻辑性</li> <li>给对方思考的时间</li> </ul> <blockquote> <p><strong>个人反思</strong>：面试中发现一个严重的问题 —— 当面试官沉默时，我会感到尴尬，然后不停地说话填补沉默。这让我想起奥巴马关于”第一份工作面试”的演讲。我需要学会：</p> <ul> <li>适时停顿，给双方思考的空间</li> <li>组织好语言再回答</li> <li>不要因为沉默而慌乱</li> </ul> </blockquote> <h2 id="3-改进计划">3. 改进计划</h2> <h3 id="31-项目准备">3.1 项目准备</h3> <ul> <li>准备3-4个核心项目的STAR描述</li> <li>每个项目准备5分钟和15分钟两个版本</li> <li>录音练习，反复修改到逻辑清晰</li> </ul> <h3 id="32-技术准备">3.2 技术准备</h3> <ul> <li>深入学习RLHF的原理和实现细节</li> <li>每天保持算法题练习，特别是基础题目</li> <li>准备一个技术知识树，确保每个分支都能讲清楚</li> </ul> <h3 id="33-面试技巧">3.3 面试技巧</h3> <ul> <li>练习如何优雅地处理沉默</li> <li>通过录音分析自己的表达方式</li> <li>模拟面试练习，特别是压力面试情况</li> </ul> <h2 id="结语">结语</h2> <p>这次上海人工智能实验室的面试虽然不尽如人意，但给了我深刻的教训和改进的方向。面试不仅是展示技术能力的机会，更是展示一个人如何在压力下思考和表达的窗口。希望通过分享这些经验和教训，能帮助大家在面试中更好地发挥。</p>]]></content><author><name></name></author><category term="career"/><category term="面试经验"/><category term="算法工程师"/><category term="大模型"/><category term="AI/ML"/><category term="求职"/><category term="上海人工智能实验室"/><summary type="html"><![CDATA[总结面试要点，并结合上海人工智能实验室评测大模型算法工程师岗位的面试经历，分享深刻教训]]></summary></entry></feed>