<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="cn"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://1587causalai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://1587causalai.github.io/" rel="alternate" type="text/html" hreflang="cn"/><updated>2025-01-30T21:59:15+08:00</updated><id>https://1587causalai.github.io/feed.xml</id><title type="html">Teach AI Causes and Effects</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">如何高效使用 DeepSeek-R1? 一些提示词工程本质思考</title><link href="https://1587causalai.github.io/blog/2025/deepseek-prompt/" rel="alternate" type="text/html" title="如何高效使用 DeepSeek-R1? 一些提示词工程本质思考"/><published>2025-01-29T00:00:00+08:00</published><updated>2025-01-29T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/deepseek-prompt</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/deepseek-prompt/"><![CDATA[<p>在当今人工智能领域，一场来自东方的开源风暴正席卷全球，DeepSeek-R1 模型的横空出世，犹如破晓之光，照亮了 AI 发展的新路径。中国创业公司深度求索凭借这一创新力作，不仅在技术性能上与顶尖商业模型并驾齐驱，更以其显著的成本优势，引发各界广泛关注，从美国硅谷的科技巨头到华尔街的投资精英，目光纷纷聚焦于此，其应用程序在全球各国 App Store 免费榜单上的迅猛攀升，更是彰显了其强大的市场吸引力。</p> <h2 id="一deepseek-r1技术突破与全球反响">一、DeepSeek-R1：技术突破与全球反响</h2> <p>DeepSeek-R1 模型在多个关键领域展现出卓越性能，数学运算的精准、编程逻辑的严谨以及自然语言推理的流畅，都使其成为行业焦点。尤为值得一提的是，其成本仅为 OpenAI o1 模型的三十分之一，这一成本的大幅降低，无疑为高性能 AI 模型的广泛应用扫清了障碍，让更多企业和开发者能够触及前沿 AI 技术，开启创新之旅。</p> <p>其独特的训练方法更是引发热议，完全依赖强化学习（RL），摒弃了传统的监督微调（SFT），这一创新之举赋予了模型更强的泛化能力。在训练过程中，模型通过不断试错、自我优化，如同人类学习成长般，逐步提升推理能力，而非受限于预先定义的固定任务和格式套路，从而在多个基准测试中脱颖而出，甚至在某些领域超越了 OpenAI 的产品，为 AI 技术发展提供了全新思路。 这一突破性进展在全球范围内激起千层浪。美国前总统特朗普将其视为美国科技行业的 “警钟”，呼吁加大竞争力度，以应对来自东方的强劲挑战。OpenAI 首席执行官萨姆・阿尔特曼虽表示赞赏，但也深知竞争压力，承诺加快新模型发布步伐。而特斯拉 CEO 埃隆・马斯克则对 DeepSeek 的成本声称持怀疑态度，暗示背后可能存在更多高端芯片的隐秘投入。受此影响，技术股市场波动剧烈，纳斯达克指数在发布当天下跌 3.1%，英伟达股价暴跌近 17%，投资者开始重新审视美国公司在 AI 数据中心和基础设施领域的投资布局，美国科技巨头的市场主导地位面临前所未有的挑战。</p> <h2 id="二高效使用-deepseek-r1提示词工程的返璞归真">二、高效使用 DeepSeek-R1：提示词工程的返璞归真</h2> <p>在这场技术变革浪潮中，作为实际使用者，我们最为关切的无疑是如何真正驾驭 DeepSeek-R1 这一强大模型，让其为我们的工作、学习和生活赋能。在传统大模型应用中，用户常常陷入 “咒语设计” 的困境，<strong>精心雕琢思维链（CoT）、角色扮演（Role-Play）、格式模板等提示技巧</strong>，试图通过复杂繁琐的指令来获取理想答案。然而，DeepSeek-R1 的出现打破了这一固有模式，它宛如一位更懂用户的知心伙伴，无需繁复修饰，只需我们直接提问，便能给出精准回应。</p> <p>有趣的是，广为流传的一个提示词是 “说人话”，这看似简单的话语，实则道出了 DeepSeek-R1 与众不同的特质。它更擅长与人类进行自然流畅的沟通，摒弃了以往复杂提示词如同魔法咒语般的晦涩难懂，真正回归到人机协作的本质。而这一切，得益于其构建时的创新理念，即原始地采用强化学习（RL）而非监督微调（SFT）方法。</p> <p>在主流大模型构建中，监督微调（SFT）被视为常规路径，通过预先定义大量任务和格式套路，让模型去学习模仿。但 DeepSeek-R1 另辟蹊径，坚信大模型应仿效人类学习方式，借助强化学习，在不断试错中优化自身，从而具备更强的泛化能力，不被固定任务和格式所束缚。这一理念的转变，为我们揭示了<strong>使用 AI 的 “第一性原理” —— 说清楚需求，定义好问题，提供好上下文:</strong></p> <ul> <li>你的需求是什么? 合适的定义任务和问题是和 AI 协作的关键第一步, 当需求很复杂的时候, 我们需要进行适当的拆解.</li> <li>问题背景是什么? 各种问题背景信息, 用户经验和偏好信息等一起提供给模型, 模型会基于这些信息给出更好的答案.</li> </ul> <p>当我们与 DeepSeek-R1 交互时，无需再费心琢磨那些花哨的提示技巧，而是将精力聚焦于明确自身需求。例如，在寻求写作灵感时，直接告知模型 “我需要一篇关于环保主题的文章大纲，要求涵盖当前环境问题、解决方案以及未来展望，字数在 1000 字左右”，如此清晰明确的需求描述，远胜于复杂的思维链引导。同时，定义好问题至关重要，若你是一名程序员，遇到代码报错，精准地向模型阐述 “我在使用 Python 编写爬虫程序时，遇到了 ‘requests’ 库连接超时的问题，已尝试过检查网络连接和服务器状态，但仍未解决，该如何排查？” 这样具体且聚焦的问题，能让模型迅速定位关键点，给出有效建议。此外，提供好上下文也不可或缺，若你正在进行学术研究，向模型展示相关文献资料、研究背景等上下文信息，它便能基于此为你拓展思路、补充论据。</p> <h2 id="三总结">三、总结</h2> <p>DeepSeek-R1 的诞生，无疑是人工智能发展史上的一座里程碑。它不仅在技术层面实现了重大突破，更在人机交互理念上引发深刻变革。对于广大用户而言，掌握其使用方法的关键，在于深刻领悟提示词工程的本质 —— 说清楚需求，定义好问题，提供好上下文。当我们摒弃形式主义的复杂技巧，回归到与 AI 简单直接、清晰准确的沟通时，DeepSeek-R1 将成为我们最得力的助手，助力我们在 AI 时代开启高效智能的协作之旅，解锁无限可能。</p> ]]></content><author><name></name></author><category term="prompt"/><category term="DeepSeek-R1"/><category term="Prompt-Engineering"/><category term="LLM"/><category term="AI"/><summary type="html"><![CDATA[当普通大模型用户沉迷于魔法咒语时，真正的高手早已返璞归真。那些复杂的思维链模板、角色扮演话术，往往让需求迷失在形式主义中。与AI协作的道，在于说清楚需求, 定义好问题, 提供好上下文.]]></summary></entry><entry><title type="html">LlamaFactory 使用教程：轻松实现大模型微调</title><link href="https://1587causalai.github.io/blog/2025/llama-factory-tutorial/" rel="alternate" type="text/html" title="LlamaFactory 使用教程：轻松实现大模型微调"/><published>2025-01-04T00:00:00+08:00</published><updated>2025-01-04T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2025/llama-factory-tutorial</id><content type="html" xml:base="https://1587causalai.github.io/blog/2025/llama-factory-tutorial/"><![CDATA[<p>各位朋友，今天我必须、一定要、强烈地安利一个神奇的工具——<strong>LlamaFactory</strong>！如果你和我一样，对微调大型语言模型充满热情，但又被各种复杂的命令行参数和环境配置搞得头大，那么请你务必认真看完这篇文章。因为，它真的会让你尖叫！</p> <p>我这篇文章将理论结合实践来写，通过两个具体步骤来展示 LlamaFactory 的强大：</p> <ol> <li>最快速地跑一个最简单的 DPO，体现它的易用性</li> <li>使用本地模型和本地数据进行 DPO，体现它的定制性</li> </ol> <h2 id="初见-llamafactory惊艳的第一印象">初见 LlamaFactory：惊艳的第一印象</h2> <p>我之前也尝试过一些微调工具，要么就是配置起来像解一道高数题，要么就是界面简陋得让人想砸键盘。但是，LlamaFactory-CLI WebUI 的出现，简直就像一道耀眼的光芒，瞬间照亮了我略显昏暗的 AI 炼丹之路！</p> <p>让我们从最简单的安装开始：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone <span class="nt">--depth</span> 1 https://github.com/hiyouga/LLaMA-Factory.git
<span class="nb">cd </span>LLaMA-Factory
pip <span class="nb">install</span> <span class="nt">-e</span> <span class="s2">".[torch]"</span>
</code></pre></div></div> <p>安装完成后，只需一行命令就能启动这个神奇的界面：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llamafactory-cli webui
</code></pre></div></div> <p><strong>打开浏览器，映入眼帘的界面让我惊呆了！</strong></p> <p>简洁明了的布局瞬间让人心情舒畅，各种参数选项排列有序，色彩搭配也恰到好处，完全没有那种”工程师风格”的冰冷感。这绝对是我见过最漂亮的 CLI WebUI 之一，用起来赏心悦目，效率都感觉提升了不少！</p> <h2 id="dpo-训练实战从理论到实践的完美演绎">DPO 训练实战：从理论到实践的完美演绎</h2> <p>别看界面漂亮，功能可一点都不含糊！我马上开始了一次实战训练，选择了最简单但也最能体现特点的配置：</p> <ul> <li>模型：<code class="language-plaintext highlighter-rouge">Qwen/Qwen-1_8B</code>（轻量级但效果不错的中文模型）</li> <li>数据集：<code class="language-plaintext highlighter-rouge">hh_rlhf_en</code>（内置的人类反馈数据集）</li> <li>训练方法：DPO（Direct Preference Optimization）</li> </ul> <p>点击”开始训练”后，眼前的画面让我感动得想哭：</p> <div style="text-align: center;"> <img src="https://s2.loli.net/2025/01/04/mu69nbXC7gkjowR.png" alt="训练界面" style="max-width: 85%; height: auto;"/> </div> <p><strong>这个训练过程简直就是一场视觉盛宴！</strong></p> <ul> <li>Loss 曲线平滑下降，就像一个优雅的舞者</li> <li>学习率的调整精准而富有节奏</li> <li>训练进度条稳步前进，让人心里踏实</li> <li>GPU 显存使用情况一目了然，再也不用担心爆显存</li> </ul> <p>更让我觉得贴心的是，WebUI 还贴心地生成了完整的训练命令：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llamafactory-cli train <span class="se">\</span>
    <span class="nt">--stage</span> dpo <span class="se">\</span>
    <span class="nt">--do_train</span> True <span class="se">\</span>
    <span class="nt">--model_name_or_path</span> Qwen/Qwen-1_8B <span class="se">\</span>
    <span class="nt">--preprocessing_num_workers</span> 16 <span class="se">\</span>
    <span class="nt">--finetuning_type</span> lora <span class="se">\</span>
    <span class="nt">--template</span> default <span class="se">\</span>
    <span class="nt">--flash_attn</span> auto <span class="se">\</span>
    <span class="nt">--dataset_dir</span> data <span class="se">\</span>
    <span class="nt">--dataset</span> hh_rlhf_en <span class="se">\</span>
    <span class="nt">--cutoff_len</span> 2048 <span class="se">\</span>
    <span class="nt">--learning_rate</span> 5e-05 <span class="se">\</span>
    <span class="nt">--num_train_epochs</span> 1.0 <span class="se">\</span>
    <span class="nt">--max_samples</span> 1000 <span class="se">\</span>
    <span class="nt">--per_device_train_batch_size</span> 2 <span class="se">\</span>
    <span class="nt">--gradient_accumulation_steps</span> 4 <span class="se">\</span>
    <span class="nt">--lr_scheduler_type</span> cosine <span class="se">\</span>
    <span class="nt">--max_grad_norm</span> 1.0 <span class="se">\</span>
    <span class="nt">--logging_steps</span> 5 <span class="se">\</span>
    <span class="nt">--save_steps</span> 100 <span class="se">\</span>
    <span class="nt">--warmup_steps</span> 0 <span class="se">\</span>
    <span class="nt">--packing</span> False <span class="se">\</span>
    <span class="nt">--report_to</span> none <span class="se">\</span>
    <span class="nt">--output_dir</span> saves/Qwen-1.8B/lora/train_2025-01-04-12-04-07 <span class="se">\</span>
    <span class="nt">--bf16</span> True <span class="se">\</span>
    <span class="nt">--plot_loss</span> True <span class="se">\</span>
    <span class="nt">--trust_remote_code</span> True <span class="se">\</span>
    <span class="nt">--ddp_timeout</span> 180000000 <span class="se">\</span>
    <span class="nt">--include_num_input_tokens_seen</span> True <span class="se">\</span>
    <span class="nt">--optim</span> adamw_torch <span class="se">\</span>
    <span class="nt">--lora_rank</span> 8 <span class="se">\</span>
    <span class="nt">--lora_alpha</span> 16 <span class="se">\</span>
    <span class="nt">--lora_dropout</span> 0 <span class="se">\</span>
    <span class="nt">--lora_target</span> all <span class="se">\</span>
    <span class="nt">--pref_beta</span> 0.1 <span class="se">\</span>
    <span class="nt">--pref_ftx</span> 0 <span class="se">\</span>
    <span class="nt">--pref_loss</span> sigmoid
</code></pre></div></div> <p>这个命令不仅仅是一串参数的组合，它展示了 LlamaFactory 对微调过程的深刻理解：使用 LoRA 降低显存占用，采用较小的学习率避免破坏预训练知识，设置合理的 batch size 和梯度累积步数…每个参数都经过精心调优。</p> <h2 id="定制化训练从示例到本地模型">定制化训练：从示例到本地模型</h2> <p>这个预览命令功能真的是太贴心了！有了它，我们就可以轻松地将这个训练过程迁移到自己的本地模型上。我用的是本地的 InternLM2-1.8B 模型，只需要将命令中的模型路径改为本地路径：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llamafactory-cli train <span class="se">\</span>
    <span class="nt">--stage</span> dpo <span class="se">\</span>
    <span class="nt">--do_train</span> True <span class="se">\</span>
    <span class="nt">--model_name_or_path</span> /path/to/internlm2-1.8b <span class="se">\</span>
    <span class="nt">--preprocessing_num_workers</span> 16 <span class="se">\</span>
    <span class="nt">--finetuning_type</span> lora <span class="se">\</span>
    <span class="nt">--template</span> default <span class="se">\</span>
    <span class="nt">--flash_attn</span> auto <span class="se">\</span>
    <span class="nt">--dataset_dir</span> data <span class="se">\</span>
    <span class="nt">--dataset</span> hh_rlhf_en <span class="se">\</span>
    <span class="nt">--cutoff_len</span> 2048 <span class="se">\</span>
    <span class="nt">--learning_rate</span> 5e-05 <span class="se">\</span>
    <span class="nt">--num_train_epochs</span> 1.0 <span class="se">\</span>
    <span class="nt">--max_samples</span> 1000 <span class="se">\</span>
    <span class="nt">--per_device_train_batch_size</span> 2 <span class="se">\</span>
    <span class="nt">--gradient_accumulation_steps</span> 4 <span class="se">\</span>
    <span class="nt">--lr_scheduler_type</span> cosine <span class="se">\</span>
    <span class="nt">--max_grad_norm</span> 1.0 <span class="se">\</span>
    <span class="nt">--logging_steps</span> 5 <span class="se">\</span>
    <span class="nt">--save_steps</span> 100 <span class="se">\</span>
    <span class="nt">--warmup_steps</span> 0 <span class="se">\</span>
    <span class="nt">--packing</span> False <span class="se">\</span>
    <span class="nt">--report_to</span> none <span class="se">\</span>
    <span class="nt">--output_dir</span> saves/Qwen-1.8B/lora/train_2025-01-04-12-04-07 <span class="se">\</span>
    <span class="nt">--bf16</span> True <span class="se">\</span>
    <span class="nt">--plot_loss</span> True <span class="se">\</span>
    <span class="nt">--trust_remote_code</span> True <span class="se">\</span>
    <span class="nt">--ddp_timeout</span> 180000000 <span class="se">\</span>
    <span class="nt">--include_num_input_tokens_seen</span> True <span class="se">\</span>
    <span class="nt">--optim</span> adamw_torch <span class="se">\</span>
    <span class="nt">--lora_rank</span> 8 <span class="se">\</span>
    <span class="nt">--lora_alpha</span> 16 <span class="se">\</span>
    <span class="nt">--lora_dropout</span> 0 <span class="se">\</span>
    <span class="nt">--lora_target</span> all <span class="se">\</span>
    <span class="nt">--pref_beta</span> 0.1 <span class="se">\</span>
    <span class="nt">--pref_ftx</span> 0 <span class="se">\</span>
    <span class="nt">--pref_loss</span> sigmoid
</code></pre></div></div> <p>训练完成后，在 <code class="language-plaintext highlighter-rouge">saves/internlm2-1.8b/lora/dpo_train/</code> 目录下生成了训练结果：</p> <ul> <li><code class="language-plaintext highlighter-rouge">training_loss.png</code>：训练损失曲线</li> <li><code class="language-plaintext highlighter-rouge">training_rewards_accuracies.png</code>：奖励和准确率曲线</li> <li><code class="language-plaintext highlighter-rouge">adapter_model.safetensors</code>：训练得到的 LoRA 权重</li> <li>其他配置文件和中间检查点</li> </ul> <p>训练损失曲线:</p> <div style="text-align: center;"> <img src="https://s2.loli.net/2025/01/04/XDZa4zP8sL6pH7f.png" alt="训练损失曲线" style="max-width: 85%; height: auto;"/> </div> <p>LlamaFactory 确实是一个非常好用的工具，它让模型微调变得如此简单。通过实践我发现，它的 WebUI 不仅让操作变得直观，更重要的是帮助我们理解了微调过程中的各个参数和步骤。不过需要注意的是，在实际使用中，数据质量和模型选择仍然是最关键的因素。工具再好，也要有优质的数据和合适的基座模型才能训练出好的效果。更多解读理解文档请参考 <a href="https://1587causalai.github.io/llama_factory/">link</a>。</p>]]></content><author><name></name></author><category term="ml-engineering"/><category term="llm"/><category term="fine-tuning"/><summary type="html"><![CDATA[本文介绍如何使用 LlamaFactory 进行大模型微调，包括使用 WebUI 和命令行两种方式]]></summary></entry><entry><title type="html">从个体偏见到群体智慧：人类认知的算力限制与演化</title><link href="https://1587causalai.github.io/blog/2024/human-bias-and-interview/" rel="alternate" type="text/html" title="从个体偏见到群体智慧：人类认知的算力限制与演化"/><published>2024-12-25T00:00:00+08:00</published><updated>2024-12-25T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/human-bias-and-interview</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/human-bias-and-interview/"><![CDATA[<h1 id="人类偏见统一理论与面试感悟">人类偏见统一理论与面试感悟</h1> <h2 id="人类偏见的统一理论">人类偏见的统一理论</h2> <p>在心理学领域，我们经常听到各种各样的认知偏误：确认偏误、锚定效应、损失厌恶、过度自信偏差、聚光灯效应等等。这些偏误就像成语典故一样，已经成为现代人必须了解的思维模型。但是，它们之间是否存在某种内在的联系？</p> <p>最近，两位德国心理学家艾琳·厄伯斯特（Aileen Oeberst）和罗兰·因霍夫（Roland Imhoff）提出了一个令人耳目一新的统一理论：</p> <p><strong>任何偏见 = 一个信念 + 确认偏误</strong></p> <p>这个简单而优雅的公式背后，隐藏着人类思维的基本模式。让我们深入理解这个理论：</p> <h3 id="六个根本信念">六个根本信念</h3> <ol> <li> <p><strong>“我的经验是合理的参考”</strong></p> <ul> <li>聚光灯效应：我们总觉得别人在关注自己</li> <li>透明度错觉：认为自己的想法对他人来说显而易见</li> <li>虚假共识效应：默认自己的信念是大众共识</li> </ul> </li> <li> <p><strong>“我对世界的判断是正确的”</strong></p> <ul> <li>敌意媒体偏误：认为不利于自己的中立报道是故意抹黑</li> <li>偏误盲点：容易发现他人的偏误，却看不到自己的</li> </ul> </li> <li> <p><strong>“我是好的”</strong></p> <ul> <li>优于平均水平效应：认为自己各方面能力都高于平均</li> <li>自利归因偏差：成功归因于自己，失败归因于外部</li> </ul> </li> <li> <p><strong>“我的群体是合理的参考”</strong></p> <ul> <li>种族中心主义：以自己群体的标准评判其他群体</li> <li>群体内投射：认为自己群体更能代表普遍性</li> </ul> </li> <li> <p><strong>“我们群体是好的”</strong></p> <ul> <li>内群体偏好：对自己群体的无条件认同</li> <li>外群体贬低：对其他群体的本能排斥</li> </ul> </li> <li> <p><strong>“人们的属性决定结果”</strong></p> <ul> <li>基本归因谬误：过分强调个人特质，忽视环境因素</li> </ul> </li> </ol> <p>理解了这些根本信念后，一个自然的问题是：为什么人类会形成这些系统性的偏见？这就需要我们从更基础的角度 - 计算复杂性的视角来思考。</p> <h3 id="偏见的本质算力限制与群体智慧">偏见的本质：算力限制与群体智慧</h3> <p>从计算复杂性的角度来看，Stephen Wolfram提出了一个发人深省的观点：人类作为一个智能系统，面临着根本的算力限制。就像热力学第二定律、相对论、量子力学这些物理定律可能是我们认知局限的产物一样，个体的心理偏误也可能源于我们处理信息能力的限制。</p> <p>面对复杂系统，每个个体都不得不使用简化模型来理解世界。这些简化模型就表现为各种认知偏误。但有趣的是，正是这些看似”有缺陷”的个体，在组成群体后反而展现出了更强的泛化能力。</p> <h3 id="从随机实验到群体智慧一个演化视角">从随机实验到群体智慧：一个演化视角</h3> <p>群体智慧的形成过程可以从三个层次来理解：</p> <ol> <li> <p><strong>个体层面的简化与偏见</strong></p> <ul> <li>每个人的偏见都是对世界的一种简化建模</li> <li>这种简化源于算力和信息处理能力的限制</li> <li>不同个体会形成不同的简化模型</li> </ul> </li> <li> <p><strong>群体层面的随机实验</strong></p> <ul> <li>每个个体的偏见都是一次”随机实验”</li> <li>不同的偏见产生不同的决策结果</li> <li>群体通过这些实验积累经验</li> <li>某些偏见会被证明更有适应性</li> </ul> </li> <li> <p><strong>演化层面的智慧涌现</strong></p> <ul> <li>能够识别并克服特定偏见的个体获得优势</li> <li>这种优势推动了群体的整体进化</li> <li>群体在这个过程中建立更准确的因果模型</li> <li>“偏见多样性”提供了系统的鲁棒性</li> </ul> </li> </ol> <p>这个过程揭示了一个反直觉的真理：</p> <ul> <li>极致效率的系统往往是脆弱的</li> <li>看似”低效”的偏见多样性反而提供了适应能力</li> <li>个体通过意识到偏见，既促进自身发展，也推动群体进化</li> </ul> <p>这些根本信念本质上都指向一个核心：<strong>“我跟别人很不一样，对此我感到骄傲”</strong>。这种”我执”不仅仅是认知上的偏差，更是人类社会发展的重要动力。从计算的角度看，这种差异性恰恰是群体智慧得以形成的基础。</p> <h2 id="面试场景中的偏见一些个人感悟">面试场景中的偏见：一些个人感悟</h2> <p>面试本质上是一个排序问题, 有一些基本的规则。在经历了多次面试后，我对面试场景中这些规律有了更深的体会。</p> <h3 id="算法题考察的两难">算法题考察的两难</h3> <p>最近的面试经历让我深深体会到这个矛盾：虽然在这个人工智能时代，用算法题去考察一个候选人的能力似乎已经不太合适了，但是还有比它更好的普适性考察方式吗？</p> <p>这个矛盾背后其实反映了更深层的问题：</p> <ul> <li>面试官需要一个相对客观的排序标准</li> <li>在有限时间内，算法题仍是最”公平”的量化指标</li> <li>这本身就反映了人类对”可量化标准”的偏好</li> </ul> <p>面试官的决策困境. 在面试中，我注意到一个有趣的现象：即使某个面试官看到了你独特的优势，他也未必能超越常规的评判标准给出录取决定。因为：</p> <ul> <li>作出非同寻常的判断，需要非同寻常的证据</li> <li>面试官需要承担更大的决策风险</li> <li>这反映了”损失厌恶”这一典型偏见</li> </ul> <h3 id="关于空白期的对话">关于空白期的对话</h3> <p>在东方财富的面试中，我遇到了一个看似棘手但实际是送分的问题：”你为什么gap这么久？”</p> <p>我的回答应该是：</p> <ul> <li>“我下定决心想要转向大模型方向”</li> <li>“因为背景不太匹配，所以需要花很多时间从零开始”</li> <li>“我系统化地训练了相关技能”</li> <li>“实际落地了大模型相关项目”</li> </ul> <p>我这个送分题没有回答好, 算是惨痛教训吧. 这个经历让我明白尊重面试场景的重要性：</p> <ul> <li>强调主动学习和成长</li> <li>展示明确的职业规划</li> <li>用具体项目证明能力</li> </ul> <h2 id="结语">结语</h2> <p>理解人类偏见的统一理论，不仅帮助我们认识自己的思维局限，也让我们更好地理解和尊重面试场景的特殊性。在AI时代，这些”偏见”恰恰彰显了人性的独特之处。</p> <p>正如格雷戈里·贝特森所说的”分裂演化”现象，人类社会的多样性正是建立在这些偏见之上。这种多样性虽然可能导致一些效率损失，但却是创新和进步的源泉。</p> <p>在面对偏见时，我们需要：</p> <ol> <li>认识到偏见的普遍性和必然性</li> <li>理解偏见背后的深层原因</li> <li>在保持个性的同时，尊重场景规则</li> <li>用建设性的方式应对偏见带来的挑战</li> </ol> <p>最终，无论是克服偏见还是准备面试，关键都在于保持开放和谦逊的心态。认识到自己的局限，尊重场景的规则，这样才能获得真正的进步。</p>]]></content><author><name></name></author><category term="psychology"/><category term="interview-experience"/><category term="reflection"/><summary type="html"><![CDATA[关于人类偏见的统一理论思考以及面试场景的规则]]></summary></entry><entry><title type="html">AI-Powered 项目管理：学术严谨性 + 知识普及性 + 代码快速实现</title><link href="https://1587causalai.github.io/blog/2024/project-workflow/" rel="alternate" type="text/html" title="AI-Powered 项目管理：学术严谨性 + 知识普及性 + 代码快速实现"/><published>2024-12-23T18:00:00+08:00</published><updated>2024-12-23T18:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/project-workflow</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/project-workflow/"><![CDATA[<p>上个月我写了一篇关于如何用 Overleaf 和 Cursor 来提升论文写作效率的文章。这个月的实践让我对整个研究工作流有了更深层的思考 —— 这不仅仅是工具的选择，而是关于如何更好地创造和传播知识。</p> <h2 id="为什么需要这样的工作方式">为什么需要这样的工作方式？</h2> <p>在当今这个信息爆炸的时代，我们面临着三个核心挑战：</p> <ul> <li>如何保证研究的专业性和可信度</li> <li>如何让复杂的研究更容易被理解和应用</li> <li>如何在保证质量的同时提高工作效率</li> </ul> <p>经过反复实践，我找到了一个可能的答案：将人类最严谨的知识创造方式（学术研究）、最有效的知识传播方式（通俗化表达）和最先进的效率工具（AI辅助）结合起来。 一个案例:</p> <h2 id="这个工作流程的三个支柱">这个工作流程的三个支柱</h2> <h3 id="第一支柱学术论文的严谨性">第一支柱：学术论文的严谨性</h3> <p>学术论文是人类历史上最可信的表达观点的方式。通过严格的同行评议、完整的推导论证和规范的引用体系，它确保了知识的可靠性和可追溯性。在我的工作流程中，这体现为：</p> <ul> <li>用 Overleaf（或其他 LaTeX 编辑器）来写作论文</li> <li>遵循学术规范，确保每个观点都有充分的论证</li> <li>通过版本控制保存思维的演进过程</li> </ul> <h3 id="第二支柱博客的普及性">第二支柱：博客的普及性</h3> <p>博客（或其他形式的通俗文档）是连接专业研究和普通读者的桥梁。通过 Docsify、ReadTheDocs 等工具，我们可以：</p> <ul> <li>用通俗易懂的语言解释复杂的概念</li> <li>记录研究过程中的思考和灵感</li> <li>让研究成果能被更广泛的受众理解和应用</li> </ul> <h3 id="第三支柱ai-的效率提升">第三支柱：AI 的效率提升</h3> <p>AI 工具（如 Cursor、WindSurf 等）代表了现代科技对效率的追求。它们帮助我们：</p> <ul> <li>快速实现想法，验证假设</li> <li>自动化重复性工作</li> <li>在不牺牲质量的前提下加速整个研究过程</li> </ul> <h2 id="如何实现这个工作流一个案例">如何实现这个工作流？一个案例</h2> <p>具体实施时，我选择以一个统一的项目仓库为核心：</p> <ol> <li>首先在 Overleaf 创建项目（可以与 GitHub 同步）</li> <li>在项目中建立清晰的结构： <ul> <li><code class="language-plaintext highlighter-rouge">paper/</code>: 严谨的学术论文</li> <li><code class="language-plaintext highlighter-rouge">docs/</code>: 通俗化的项目文档（可以用任何适合的文档工具部署）</li> <li><code class="language-plaintext highlighter-rouge">src/</code>: 相关的代码实现</li> </ul> </li> <li>使用现代 AI 工具辅助整个过程</li> </ol> <p>日常工作流程：</p> <ul> <li>在论文中严谨地论证核心观点</li> <li>在文档中用通俗语言分享想法和进展</li> <li>通过代码将想法落地为实际应用</li> <li>让 AI 工具贯穿始终，提供智能辅助</li> </ul> <p>用研究项目的严谨框架来驱动一个可能演变的学习/科普项目, 一个符合这个理念的结构：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.
├── paper/                      # 核心研究论文相关
│   ├── main.tex               # 论文主体
│   ├── figures/               # 论文图表
│   └── data/                  # 实验数据
│
├── docs/                       # 文档网站（精简核心）
│   ├── overview/              # 项目概览
│   │   └── introduction.md    # 项目介绍
│   ├── progress/              # 项目进展
│   │   └── research-log.md    # 研究日志
│   ├── tutorials/             # 教程（可扩展）
│   └── references/            # 参考资料
│
├── research/                   # 研究过程文档 (内部使用)
│   ├── literature/            # 文献阅读笔记
│   ├── methodology/           # 研究方法设计
│   └── analysis/             # 数据分析记录
│
└── src/                       # 代码实现
    ├── core/                  # 核心实验代码
    ├── analysis/             # 数据分析代码
    └── examples/             # 示例代码（可扩展为教程代码）
</code></pre></div></div> <p>这个设计的特点：</p> <ol> <li> <p><strong>研究驱动</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">paper/</code> 作为项目的核心，确保学术严谨性</li> <li><code class="language-plaintext highlighter-rouge">research/</code> 存放详细的研究过程文档，便于追踪思路</li> </ul> </li> <li> <p><strong>灵活演变</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">docs/</code> 采用模块化结构，可以根据项目性质逐步扩展</li> <li>从研究发现到教程的自然过渡</li> <li>代码结构支持从实验到教学的转换</li> </ul> </li> <li><strong>双层文档</strong> <ul> <li><code class="language-plaintext highlighter-rouge">docs/</code>: 精炼的对外窗口，展示核心内容</li> <li><code class="language-plaintext highlighter-rouge">research/</code>: 详细的内部文档，记录完整研究过程</li> </ul> </li> <li><strong>渐进式扩展</strong> <ul> <li>初期专注于研究部分</li> <li>中期可以增加实验结果分析</li> <li>后期可以扩展为教程和科普内容</li> </ul> </li> </ol> <p>这个结构的优势是：</p> <ol> <li>保持了研究项目的严谨框架</li> <li>提供了清晰的对外展示窗口</li> <li>支持项目性质的自然演变</li> <li>内外文档分离，既保证了完整性又保持了对外简洁</li> </ol> <h2 id="这种方式的深远意义">这种方式的深远意义</h2> <p>这个工作流程的精髓在于它完美平衡了三个关键要素：</p> <ul> <li>学术论文确保了知识的可靠性</li> <li>通俗文档保证了知识的可及性</li> <li>AI 工具带来了前所未有的效率</li> </ul> <p>这不仅仅是一个工具的选择问题，而是关于如何在人工智能时代更好地创造和传播知识的思考。它让我们能够既保持学术的严谨，又不失传播的普及性，同时还能享受现代技术带来的效率提升。</p> <h2 id="写在最后">写在最后</h2> <p>这种工作方式代表了我对未来研究工作的思考：如何让专业研究既严谨又亲民，如何让知识创造既高效又可靠。它可能不是完美的答案，但却是一个值得尝试的方向。</p> <p>如果你也在思考如何更好地进行研究工作，欢迎分享你的想法！</p>]]></content><author><name></name></author><category term="tools"/><category term="research"/><category term="workflow"/><category term="AI"/><category term="knowledge-sharing"/><summary type="html"><![CDATA[一个融合学术论文、项目文档和代码实现的 AI-Powered 现代化知识创造与传播体系]]></summary></entry><entry><title type="html">大模型算法工程师面试经验与反思</title><link href="https://1587causalai.github.io/blog/2024/interview-experience/" rel="alternate" type="text/html" title="大模型算法工程师面试经验与反思"/><published>2024-12-10T00:00:00+08:00</published><updated>2024-12-10T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/interview-experience</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/interview-experience/"><![CDATA[<h1 id="大模型算法工程师面试经验与反思">大模型算法工程师面试经验与反思</h1> <h2 id="引言">引言</h2> <p>作为一名算法工程师，面试是职业生涯中不可避免的过程。本文将结合我最近在上海人工智能实验室评测大模型算法工程师岗位的面试经历，分享面试准备的要点和个人的深刻教训。</p> <h2 id="1-面试准备的关键点">1. 面试准备的关键点</h2> <h3 id="11-简历准备">1.1 简历准备</h3> <ul> <li><strong>项目经验描述</strong> <ul> <li>不要平铺直述，要用 STAR 法则（情境、任务、行动、结果）</li> <li>突出个人贡献和解决问题的能力</li> <li>用数据说话，比如：”优化后性能提升了 30%”</li> </ul> </li> </ul> <blockquote> <p><strong>个人反思</strong>：这次面试中，我在项目描述环节表现非常不理想。讲项目的时候逻辑非常混乱，没有应用STAR法则。如果按照STAR法则来组织语言，应该是这样的：</p> <ul> <li><strong>情境</strong>：介绍项目背景和面临的挑战</li> <li><strong>任务</strong>：明确说明我的职责</li> <li><strong>行动</strong>：详细描述采取的技术方案</li> <li><strong>结果</strong>：用数据说明项目成果</li> </ul> </blockquote> <ul> <li><strong>技术栈展示</strong> <ul> <li>分层次列举：熟练掌握的放在前面</li> <li>不要列举所有接触过的技术</li> <li>确保写在简历上的都能讲清楚原理</li> </ul> </li> </ul> <blockquote> <p><strong>个人反思</strong>：面试官对RLHF问了很多深入的问题，但我的回答支支吾吾。这暴露出一个问题：简历上写的技术一定要准备充分，必须能讲清楚具体原理和实现细节。</p> </blockquote> <h3 id="12-技术准备">1.2 技术准备</h3> <ul> <li><strong>必备知识点</strong> <ul> <li>计算机基础：操作系统、计算机网络、数据结构</li> <li>深度学习基础：模型结构、训练技巧、优化方法</li> <li>大模型相关：预训练、微调、RLHF</li> </ul> </li> <li><strong>算法准备</strong> <ul> <li>LeetCode 经典题目</li> <li>常见数据结构的实现</li> <li>面试高频算法题型</li> </ul> </li> </ul> <blockquote> <p><strong>个人反思</strong>：面试中遇到一道简单的链表题目，但当时大脑一片空白，没能很好地解决。这说明即使是最基础的算法题也不能掉以轻心，平时要多加练习，保持手感。</p> </blockquote> <h2 id="2-面试现场的注意事项">2. 面试现场的注意事项</h2> <h3 id="21-技术面试中的坑">2.1 技术面试中的坑</h3> <ul> <li> <p><strong>项目经验描述不够深入</strong></p> <ul> <li>❌ 错误示范：”我负责了模型训练和优化”</li> <li>✅ 正确示范：”我设计并实现了基于RLHF的对话模型优化方案，通过改进奖励模型和采样策略，将模型在人类偏好评估上的得分提升了40%”</li> </ul> </li> <li> <p><strong>技术问题回答不当</strong></p> <ul> <li>❌ 错误示范：不懂装懂，答非所问</li> <li>✅ 正确示范：诚实承认不了解，但表达学习意愿和解决问题的思路</li> </ul> </li> </ul> <h3 id="22-沟通技巧">2.2 沟通技巧</h3> <ul> <li>保持冷静</li> <li>注意表达的逻辑性</li> <li>给对方思考的时间</li> </ul> <blockquote> <p><strong>个人反思</strong>：面试中发现一个严重的问题 —— 当面试官沉默时，我会感到尴尬，然后不停地说话填补沉默。这让我想起奥巴马关于”第一份工作面试”的演讲。我需要学会：</p> <ul> <li>适时停顿，给双方思考的空间</li> <li>组织好语言再回答</li> <li>不要因为沉默而慌乱</li> </ul> </blockquote> <h2 id="3-改进计划">3. 改进计划</h2> <h3 id="31-项目准备">3.1 项目准备</h3> <ul> <li>准备3-4个核心项目的STAR描述</li> <li>每个项目准备5分钟和15分钟两个版本</li> <li>录音练习，反复修改到逻辑清晰</li> </ul> <h3 id="32-技术准备">3.2 技术准备</h3> <ul> <li>深入学习RLHF的原理和实现细节</li> <li>每天保持算法题练习，特别是基础题目</li> <li>准备一个技术知识树，确保每个分支都能讲清楚</li> </ul> <h3 id="33-面试技巧">3.3 面试技巧</h3> <ul> <li>练习如何优雅地处理沉默</li> <li>通过录音分析自己的表达方式</li> <li>模拟面试练习，特别是压力面试情况</li> </ul> <h2 id="结语">结语</h2> <p>这次上海人工智能实验室的面试虽然不尽如人意，但给了我深刻的教训和改进的方向。面试不仅是展示技术能力的机会，更是展示一个人如何在压力下思考和表达的窗口。希望通过分享这些经验和教训，能帮助大家在面试中更好地发挥。</p>]]></content><author><name></name></author><category term="career"/><category term="面试经验"/><category term="算法工程师"/><category term="大模型"/><category term="AI/ML"/><category term="求职"/><category term="上海人工智能实验室"/><summary type="html"><![CDATA[总结面试要点，并结合上海人工智能实验室评测大模型算法工程师岗位的面试经历，分享深刻教训]]></summary></entry><entry><title type="html">Let’s Dance with Causality!</title><link href="https://1587causalai.github.io/blog/2024/causalai-blueprint/" rel="alternate" type="text/html" title="Let’s Dance with Causality!"/><published>2024-11-27T00:00:00+08:00</published><updated>2024-11-27T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/causalai-blueprint</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/causalai-blueprint/"><![CDATA[<p>2022年11月30日，ChatGPT的发布标志着人工智能进入了一个新的范式：以下一个词预测（Next Token Prediction）和规模定律（Scaling Law）为核心的大语言模型时代。这个范式在短短两年内带来了令人瞩目的突破，从GPT-4到Claude 3.5，展现出大语言模型惊人的能力。然而，这个范式也逐渐显露出其根本性挑战：高质量训练数据趋于饱和、推理算力成本难以持续扩展。面对这些瓶颈，业界和学术界正在多个方向上探索突破：探索新的规模定律(例如 OpenAI o1 模型)、多模态融合、构建持续学习能力、世界模型等。在众多方向中，我选择将研究重心放在因果推理与大语言模型的融合上。这个方向虽然充满挑战，但我相信它有潜力从根本上改变当前的范式。</p> <h2 id="why-should-we-bet-on-causality">Why Should We Bet on Causality?</h2> <p>想象一下芯片技术的发展历程。在芯片领域，制程的提升是核心驱动力 - 从130nm到3nm，每一次制程的突破都带来了质的飞跃。这让我们思考：在AI领域，什么才是真正的”制程”？我认为，这个关键在于信息密度。</p> <p>当我们谈论因果关系时，我们实际上是在讨论一种极其密集的信息形式。因果之梯定理（The Causal Hierarchy Theorem）揭示了一个惊人的事实：相关性信息相对于因果信息（包括干预信息和反事实信息）实际上是一个零测集。这意味着因果模型比纯相关性模型包含了指数级更多的信息 - 就像物理定律相对于统计规律那样，是一种本质上更加密集的信息形式。</p> <p>因果思维不仅是信息密度的问题，更是人类认知的核心能力。从儿童认知发展的研究可以看到，人类天生就具备探索因果关系的倾向。如果我们希望AI能够真正理解人类意图、实现深度沟通，成为人类的得力助手，那么掌握因果思维就显得尤为重要。这也正是为什么Bengio在提出双系统理论时，将因果关系置于系统二的核心位置。</p> <p>此时, 如何让 AI 具备因果思维, 掌握因果推理, 就成为了一个非常有趣且重要的问题.</p> <h2 id="understanding-causality">Understanding Causality</h2> <p>人类思索和运用因果关系已有数千年历史。从最简单的”按下开关，灯亮了”，到复杂的”教育如何影响收入”，因果关系无处不在，是世界运行的基本机制。在不同领域中，因果关系呈现出独特的形式：数学中是”因为…所以…“的推理链条，物理学中体现为牛顿三定律等基本规律，法律领域则是”根据…可以推断…“的证据链。</p> <p>当我们谈及因果效应时，核心是 Difference-making：一个事物的改变如何导致另一个事物的变化。这与仅表示统计关联的相关性有本质区别。两个事物之间可能存在统计关联，但这并不意味着改变其中一个必然会影响另一个 - 这就是我们常说的”seeing不等同于doing”，或”相关性不等于因果关系(Correlation is not Causation)”。这也解释了为什么随机对照实验在因果推断中如此重要，以至于被称为”黄金准则”。</p> <p>因果推理其实理解起来并不容易, 比如用于观测数据因果效应估计的逆概率加权(IPW)方法, Bready Neal 教材的公式 (7.2.1):</p> \[\begin{align} \hat{\tau} &amp;= \frac{1}{n}\sum_i \left(\frac{\mathbb{1}(t_i = 1)y_i}{\hat{e}(w_i)} - \frac{\mathbb{1}(t_i = 0)y_i}{1-\hat{e}(w_i)}\right) \\ &amp;= \frac{1}{n_1}\sum_{i:t_i=1}\frac{y_i}{\hat{e}(w_i)} - \frac{1}{n_0}\sum_{i:t_i=0}\frac{y_i}{1-\hat{e}(w_i)} \end{align}\] <p>通过量纲分析可以得知第2个等号是错的, 这个错误反应大家认为只需简单直接通过倾向得分加权以后, 就可以利用观测数据估计因果效应. 可见即使是有名教材在耳熟能详的问题上也会犯误, 足以说明理解因果关系并不容易.</p> <p>更进一步, 我们有几个著名悖论需要细致理解因果关系, 才能避免犯错:</p> <ul> <li>辛普森悖论：治疗效果的迷思 加州大学伯克利分校在20世纪70年代的入学数据引发了一场争议。总体数据显示，男性申请者的录取率高于女性申请者，这看似反映了性别歧视。然而，当研究者们分院系统计时，却发现在每个院系中，女性的录取率都略高于男性。这个看似矛盾的现象就是著名的辛普森悖论。 究其原因，女性倾向于申请竞争更激烈的院系（如英语系、历史系），而男性则更多申请竞争相对较小的院系（如工程系）。这提醒我们，观察到的统计相关性可能会误导我们对因果关系的判断，必须考虑潜在的混淆因素。</li> <li>Berkson悖论：医院里的反直觉现象 想象一家医院的研究人员在研究两种疾病A和B之间的关系。他们发现，在住院病人中，患有疾病A的人较少患有疾病B，反之亦然，呈现出负相关。这个发现似乎暗示这两种疾病之间存在某种相互抑制的关系。 但实际上，这种负相关是一种统计假象。因为人们通常只有在病情较重时才会住院。如果一个人同时患有A和B两种疾病，即使每种疾病的严重程度都不足以单独导致住院，但两种疾病的叠加效应可能会导致住院。这就导致了在住院人群中观察到的负相关，这就是Berkson悖论。</li> <li>替代指标悖论：一个代价惨重的教训 1989年，辉瑞公司开发的抗心律失常药物恩卡尼（Encainide）获得了FDA的批准。这个药物之所以获批，是因为它能有效降低心电图上的室性早搏次数——这是当时普遍认为的心脏病发作风险的重要指标。 然而，后续的CAST研究揭示了一个触目惊心的事实：服用该药物的心梗后患者的死亡率反而增加了2-3倍。原来，虽然药物确实减少了早搏次数，但它同时也增加了更严重的心律失常风险。这个案例告诉我们，替代指标（室性早搏次数）的改善并不一定意味着真正关心的结果（患者存活率）会改善。这就是替代指标悖论的典型案例，它提醒我们在医学研究中必须谨慎评估因果关系。</li> </ul> <p>在众多因果性视角中，我特别关注其不变性特征：Causality is Invariance Across Different Units。就像物理定律一样，每个因果关系都有其适用范围，都是context-specific的。每个具体场景（context）定义了一个子群体，其中的个体虽然各不相同，但都遵循相同的因果关系。这启发我们将研究重点放在个体(individual/unit)的异质性（heterogeneity）和同质性（Homogeneity）上，将其作为因果研究的基本要素(Primitive), 所以我们开发了基于 individual causality 的 DiscoSCM 框架[1].</p> <h2 id="the-discoscm-framework">The DiscoSCM Framework</h2> <p>DiscoSCM（Distribution-consistency Structural Causal Models）作为基于个体因果（Individual Causality）的框架，引入了专门的随机变量U来表征个体特征。这使得描述因果机制的结构方程从传统的 \(Y = f(X, E)\) 扩展为 \(Y = f(X, E; U)\)，其中 \(X\) 是原因变量，\(E\) 是外生噪声，而 \(U\) 则表征该个体。</p> <p>这个框架一个根本的创新在于对因果结果随机性来源的重新诠释。在主流因果框架中，SCM将观察到的具体结果 \(Y=y\) 归因于外生噪声 \(E\) 的特定实现值，而 PO（Potential Outcome）框架则将其归因于特定个体的选择。DiscoSCM 融合了这两者，明确指出：观察到的具体结果 \(Y=y\) 是由外生噪声 \(E\) 的实现值和个体表征 \(U\) 的取值共同决定的。</p> <p>Variables in Counterfactual World 本质是不可观测的, 如何建立其与 Observable Variables in Factual World 的联系涉及到了因果推理的本质. DiscoSCM 这种双重决定性的观点直接启发我们对因果推理的最基础假设的反思。传统框架依赖的一致性规则（consistency rule）规定：当观察到处理 \(X=x\) 时，该特定 treatment level 的反事实结果 \(Y(x)\) 必须等于实际观察结果 \(Y\)。而DiscoSCM提出的<strong>分布一致性规则</strong>则指出：给定处理 \(X=x\) 和个体表征 \(U=u\) 时，反事实结果 \(Y(x)\) 与实际结果 \(Y\) 的分布应当一致，即：</p> \[X=x, U=u \Rightarrow Y(x) =_d Y\] <p>这两种一致性规则的区别可以通过具体例子来理解：假设我们观察到某平台上一群高补贴用户具有高留存（\(X=1, Y=1\)）。传统的consistency rule会推导出 \(Y(1)=1\)，暗示只要给这群用户高补贴就必然导致高留存。这显然忽视了一个现实：部分用户的高留存可能仅仅源于偶然因素。相比之下，Distribution-consistency 能够更准确地刻画这种情况：它承认在相同条件下，反事实结果 \(Y(1)\) 会呈现出一定的分布，从而能够区分出多少留存应归因于补贴政策，多少应归因于随机因素。这种差异在更多场景中都能体现。比如，当我们问”一个考上清华计算机系的学生，如果回到过去重新高考，有多大概率能再次考上？” 显然不会是 100%，因为即使在完全相同的条件下, 承认不可控制的 Exogenous Noise 更符合我们对现实世界的理解, 所以这类问题本质上需要 Distribution-consistency 框架来回答。请注意因果信息有三个层级(seeing, doing, imagining), 这两个例子都是 imagining 的例子, 事实上 DiscoSCM 和 SCM 在前面两个层级是数学上等价的[1].</p> <p>基于个体因果的DiscoSCM框架具有强大的解释力，能够为多个经典因果悖论提供更直观合理的解释：</p> <blockquote> <p>Simpson’s Paradox（辛普森悖论）</p> </blockquote> <p>以性别歧视为例，当我们研究性别（\(X\)）与录取结果（\(Y\)）之间的因果关系时，判断性别歧视的本质是考察对随机选取的个体（\(U\)），其在不同性别下的潜在结果\(Y(0)\)和\(Y(1)\)是否存在显著差异。然而，现实中观察到的男女群体（\(X=0,1\)）与随机个体（\(U\)）的分布并不一致，这种选择偏差导致直接计算的男女录取率并不能作为 \(Y(x)\) 的无偏估计，因此不能直接用于推断性别歧视的存在。</p> <blockquote> <p>Berkson’s Paradox（伯克森悖论）</p> </blockquote> <p>当我们关心两种疾病在一般人群中是否独立时，实际上是在考虑对随机个体而言这两个事件是否独立。然而，如果我们的数据仅来自住院病人这个特殊群体（有偏总体），那么在这个群体中观察到的负相关并不能用来推断一般人群中两种疾病的独立性。</p> <blockquote> <p>Surrogate Endpoint Paradox（替代终点悖论）</p> </blockquote> <p>在研究用药（\(X\)）、替代指标（\(S\)）和最终结果（\(Y\)）的因果链时，即使我们观察到\(X\)对\(S\)以及\(S\)对\(Y\)都有显著的因果效应，也不能直接推断\(X\)对\(Y\)存在显著因果效应。从个体因果的视角来看，这种现象可能源于人群的异质性：对某些人而言，\(X\)影响\(S\)但\(S\)不影响\(Y\)；对另一些人而言，\(X\)不影响\(S\)但\(S\)影响\(Y\)。当这种异质性存在时，即使分别观察到\(X\)到\(S\)和\(S\)到\(Y\)的因果效应，在整体人群水平上\(X\)对\(Y\)的复合因果效应也可能不显著。</p> <p>DiscoSCM框架不仅具有强大的解释力，还推导出了一系列重要的理论结果。其中一个很有意思的结论是：</p> \[\begin{align} E[Y(t)|T=t, Y=y] &amp;= E\bigg[\frac{Y \mathbb{1}_{T=t}}{P(T=t)} \cdot \frac{P(Y=y|T=t; U)}{P(Y=y|T=t)}\bigg] \end{align}\] <p>这个公式为我们提供了一个估计反事实的实用方法。比如，在前面的例子中，只要我们能够建立个体层面的模型:</p> \[\begin{align} P(Y=y|T=t;U) \end{align}\] <p>就能够准确计算出：高留存用户中有多少比例应归因于高补贴政策，或者考上清华的结果中有多少比例应归因于学生的实际能力。这种精确的归因能力，使得DiscoSCM框架不仅在理论上优雅，在实践中也具有重要价值。</p> <p>至此, 我们已经把这个新因果框架的来龙去脉介绍清楚了, 接下来将会介绍我们在这个框架下的一些人工智能应用的探索.</p> <h2 id="our-exploration-on-ai-inspired-by-causality">Our Exploration on AI Inspired by Causality</h2> <p></p> <p>如果将人工智能视作一个信息处理的自动化系统，那么信息的表征和融合就成为核心问题之一。使用概率分布来表征信息是一种常见做法，这立即引出一个关键问题：这个分布应该定义在什么样的概率空间上？DiscoSCM框架给了我们重要启发：任何证据或观察都与某个特定的子总体（sub-population）相关联。因此，我们可以用子总体来表征信息，这为信息表示提供了一个新的视角。</p> <p>我们使用这个思路来审视大语言模型（LLM）。作为一个下一个词预测器（next token predictor），LLM本质上是在给定上下文（context）的条件下，生成token空间上的一个分布。这个分布定义了token空间的一个子总体，从而完成了信息处理过程。在实际应用中，我们常常需要通过奖励信息（reward）来引导生成过程，使模型更好地对齐人类偏好。这本质上是一个信息融合问题：如何将奖励信息与模型的原始分布有效融合？</p> <p>DPO（Direct Preference Optimization）提供了一种解决方案。我们提出了一个简洁的信息融合算子 \(\odot\) 来重新诠释DPO算法的核心机制 [3]：</p> \[\begin{equation} \pi(\cdot|x) = \pi_{ref}(\cdot|x) \odot p_r(\cdot|x;\beta) \end{equation}\] <p>其中，\(p_r\) 表示对奖励函数 \(r\) 应用温度参数为 \(\beta\) 的softmax变换得到的Boltzmann探索概率分布：</p> \[\begin{equation} p_r(\cdot|x;\beta) \propto \exp(\frac{1}{\beta} r(x, \cdot)) \end{equation}\] <p>大语言模型需要对齐才能生成更符合人类偏好的内容, 自然会问是符合哪些人类偏好? 不同国家, 不同文化背景, 不同性别, 不同年龄的人类偏好显然是不同的, 所以我们更需要个性化的对齐用户偏好. 这就直接有两个大语言模型优化思路:</p> <ul> <li>把个性化加入到奖励建模, 考虑更好的 reward function design 来建模人类偏好.</li> <li>考虑因果思维链: Context –&gt; User Representation –&gt; Response, 将其融入到模型训练中.</li> <li>思考构建评测大语言模型 Implicit Personalizatin 能力的数据集.</li> </ul> <p>大语言模型的对齐问题不应该追求一个统一的”人类偏好”标准。事实上，不同国家、文化背景、性别、年龄的用户具有显著不同的价值观和偏好。这种多样性启发我们思考：如何实现大语言模型的个性化对齐？我们提出三个相互关联的研究方向：</p> <ol> <li> <p><strong>个性化奖励建模</strong>：将用户表征及其偏好特性明确纳入奖励函数设计的考虑中，使模型能够针对不同用户群体生成更适合的内容。</p> </li> <li> <p><strong>因果链条的融入</strong>：新增构建 Context → User Representation → Response 的因果思维链，将其作为模型训练的核心机制，使个性化成为模型的内在能力而非外部调整。</p> </li> <li> <p><strong>评测框架的建立</strong>：构建专门的数据集来评测模型的隐式个性化（Implicit Personalization）能力，为个性化对齐的研究提供客观的衡量标准。</p> </li> </ol> <h2 id="conclusion-and-future-work">Conclusion and Future Work</h2> <p>本文从因果推理的视角重新审视了大语言模型的发展。我们认为，当前以Next Token Prediction为核心的范式虽然取得了显著成果，但也面临着数据效率低、推理成本高等挑战。通过将因果推理与大语言模型相结合，我们看到了突破这些瓶颈的可能性。</p> <p>特别地，我们提出的DiscoSCM框架通过引入个体表征，为因果推理提供了新的视角。这个框架不仅能够优雅地解释多个经典的因果悖论，还为精确的归因分析提供了理论基础。更重要的是，这个框架启发我们思考如何更好地表征和融合信息，为大语言模型的改进提供了新的思路。</p> <p>在实践层面，我们将这些理论见解应用到了大语言模型的个性化对齐问题上。通过将用户表征纳入考虑，我们提出了包括个性化奖励建模、因果链条融入和评测框架建立在内的完整研究方案。</p> <p>展望未来，我们认为有以下几个值得深入探索的方向：</p> <ol> <li> <p><strong>因果结构的自动发现</strong>：研究如何从大规模语言数据中自动发现和学习因果结构，使模型能够更好地理解事物之间的因果关系。</p> </li> <li> <p><strong>高效的个性化机制</strong>：探索在保持模型基础能力的同时，如何实现轻量级的个性化调整，使模型能够更好地适应不同用户的需求。</p> </li> <li> <p><strong>可解释性的提升</strong>：利用因果框架提供的清晰结构，提高模型决策过程的可解释性，使我们能够更好地理解和控制模型的行为。</p> </li> <li> <p><strong>理论与实践的深度融合</strong>：进一步探索如何将因果推理的理论见解转化为实际可行的算法和架构设计。</p> </li> </ol> <p>我们相信，随着因果推理与大语言模型的深度融合，我们将能够构建出更加智能、更具个性化、也更值得信赖的AI系统。这不仅将推动技术的进步，也将帮助我们更好地理解和服务人类的多样性需求。</p> <h2 id="references">References</h2> <p>[1] Heyang Gong, Chaochao Lu, and Yu Zhang. Distribution-consistency Structural Causal Models. arXiv preprint arXiv:2401.15911. 2024.</p> <p>[2] Judea Pearl, and Dana Mackenzie. The book of why: the new science of cause and effect[M]. Basic books, 2018.</p> <p>[3] Heyang Gong. A Novel Information Fusion Framework Based on a Simple Stochastic Aggregation Operator with Applications in Decision-Making. 2024.</p>]]></content><author><name></name></author><category term="research"/><category term="causality"/><category term="machine-learning"/><category term="AI"/><category term="llm"/><summary type="html"><![CDATA[探讨因果推理的本质，思考如何让 AI 能够理解因果关系]]></summary></entry><entry><title type="html">IDE的未来发展：VSCode、Cursor等工具的演进</title><link href="https://1587causalai.github.io/blog/2024/future-of-ides/" rel="alternate" type="text/html" title="IDE的未来发展：VSCode、Cursor等工具的演进"/><published>2024-11-15T00:00:00+08:00</published><updated>2024-11-15T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/future-of-ides</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/future-of-ides/"><![CDATA[<p>在最新的一期访谈中，Anthropic的CEO Dario Amodei与AI研究者Lex Fridman就集成开发环境(IDE)的未来发展进行了深入讨论。这次对话涵盖了VSCode、Cursor等现代开发工具的发展方向，以及AI如何改变开发者的编程体验。</p> <h2 id="主要观点">主要观点</h2> <ul> <li>VSCode的成功与未来发展</li> <li>Cursor等新一代AI辅助开发工具的创新</li> <li>IDE与AI集成的趋势与挑战</li> </ul> <p>想了解更多详情，可以观看完整访谈：<a href="https://www.youtube.com/watch?v=0DjrbRQosrI">Future of IDEs: VSCode, Cursor, etc</a></p>]]></content><author><name></name></author><category term="技术资讯"/><category term="开发工具"/><category term="VSCode"/><category term="Cursor"/><category term="IDE"/><category term="开发工具"/><summary type="html"><![CDATA[在最新的一期访谈中，Anthropic的CEO Dario Amodei与AI研究者Lex Fridman就集成开发环境(IDE)的未来发展进行了深入讨论。这次对话涵盖了VSCode、Cursor等现代开发工具的发展方向，以及AI如何改变开发者的编程体验。]]></summary></entry><entry><title type="html">从外卖配送到最短路径：堆数据结构的优雅之美</title><link href="https://1587causalai.github.io/blog/2024/heap-shortest-path/" rel="alternate" type="text/html" title="从外卖配送到最短路径：堆数据结构的优雅之美"/><published>2024-11-15T00:00:00+08:00</published><updated>2024-11-15T00:00:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/heap-shortest-path</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/heap-shortest-path/"><![CDATA[<p>你有没有好奇过，外卖小哥是如何快速决定先送哪个订单？或者是导航软件是怎么在眨眼间就找到最佳路线？这些看似简单的日常问题背后，隐藏着一个优雅的数据结构 —— 堆（Heap）。</p> <h2 id="堆的直观理解">堆的直观理解</h2> <p>想象一个金字塔，最重要的事情永远放在塔尖。每完成一件事，第二重要的自动浮到顶部。这就是堆的核心思想！这个看似简单的结构，却能让 Dijkstra 的最短路径算法从复杂度 O(V²) 降低到 O((V+E)logV)。</p> <h2 id="为什么堆如此重要">为什么堆如此重要？</h2> <p>在最短路径问题中，我们需要不断地：</p> <ol> <li>找到当前最近的未访问节点</li> <li>更新与该节点相邻的节点的距离</li> </ol> <p>如果使用普通数组，每次找最小值都需要遍历整个数组，时间复杂度是 O(V)。而使用堆结构，这个操作的复杂度仅为 O(logV)！</p> <p>更令人着迷的是这种设计思想：通过精心安排数据的组织方式，看似困难的问题可以变得异常简单。这不仅仅是个算法问题，更是一种解决复杂问题的智慧。</p> <h2 id="代码实现示例">代码实现示例</h2> <p>考虑一个简单的图：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A --- 4 --- B
|           |
2           3
|           |
C --- 1 --- D
</code></pre></div></div> <p>在寻找最短路径时：</p> <ol> <li>我们需要不断选择”当前最近的未访问节点”</li> <li>节点的距离会随着算法进行不断更新</li> <li>每次都要在剩余节点中找最小距离</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dijkstra_with_heap</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">start</span><span class="p">):</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="p">{</span><span class="n">node</span><span class="p">:</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph</span><span class="p">}</span>
    <span class="n">distances</span><span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">pq</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="p">)]</span>

    <span class="k">while</span> <span class="n">pq</span><span class="p">:</span>
        <span class="c1"># 获取最小距离节点：O(log V)
</span>        <span class="n">current_distance</span><span class="p">,</span> <span class="n">current</span> <span class="o">=</span> <span class="n">heapq</span><span class="p">.</span><span class="nf">heappop</span><span class="p">(</span><span class="n">pq</span><span class="p">)</span>
        <span class="c1"># ... 更新距离
</span>        <span class="n">heapq</span><span class="p">.</span><span class="nf">heappush</span><span class="p">(</span><span class="n">pq</span><span class="p">,</span> <span class="p">(</span><span class="n">new_distance</span><span class="p">,</span> <span class="n">neighbor</span><span class="p">))</span>
    <span class="c1"># 总时间复杂度：O((V + E) log V)
</span></code></pre></div></div> <p>堆中的元素实际上代表了”候选路径”, 堆就是帮我们维护这个”待考虑的路径”清单，确保我们总是优先考虑最短的路径。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 初始状态：
</span><span class="n">堆</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)]</span>  <span class="c1"># (到A的距离=0, 节点=A)
</span><span class="n">distances</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">:</span> <span class="n">inf</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="n">inf</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">:</span> <span class="n">inf</span><span class="p">}</span>

<span class="c1"># 第1步：弹出(0, 'A')，处理A的邻居
</span><span class="n">发现</span><span class="err">：</span><span class="n">A</span><span class="o">-&gt;</span><span class="n">B</span> <span class="o">=</span> <span class="mi">4</span>
     <span class="n">A</span><span class="o">-&gt;</span><span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">堆</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)]</span>  <span class="c1"># C更近，所以在堆顶
</span><span class="n">distances</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">:</span> <span class="n">inf</span><span class="p">}</span>

<span class="c1"># 第2步：弹出(2, 'C')，处理C的邻居
</span><span class="n">检查</span><span class="err">：</span><span class="n">C</span><span class="o">-&gt;</span><span class="n">B</span> <span class="o">=</span> <span class="mi">1</span><span class="err">，</span><span class="n">发现一条更短的到B的路径</span><span class="err">：</span><span class="n">A</span><span class="o">-&gt;</span><span class="n">C</span><span class="o">-&gt;</span><span class="n">B</span> <span class="o">=</span> <span class="mi">2</span><span class="o">+</span><span class="mi">1</span> <span class="o">=</span> <span class="mi">3</span><span class="err">，</span><span class="n">比之前的4要好</span>
     <span class="n">C</span><span class="o">-&gt;</span><span class="n">D</span> <span class="o">=</span> <span class="mi">5</span><span class="err">，</span><span class="n">第一次找到到D的路径</span><span class="err">：</span><span class="n">A</span><span class="o">-&gt;</span><span class="n">C</span><span class="o">-&gt;</span><span class="n">D</span> <span class="o">=</span> <span class="mi">2</span><span class="o">+</span><span class="mi">5</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">堆</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">3</span><span class="p">,</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">)]</span>  <span class="c1"># B更近，所以在堆顶
</span><span class="n">distances</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">:</span> <span class="mi">7</span><span class="p">}</span>

<span class="c1"># 第3步：弹出(3, 'B')，处理B的邻居
</span><span class="n">检查</span><span class="err">：</span><span class="n">B</span><span class="o">-&gt;</span><span class="n">D</span> <span class="o">=</span> <span class="mi">3</span><span class="err">，</span><span class="n">发现一条更短的到D的路径</span><span class="err">：</span><span class="n">A</span><span class="o">-&gt;</span><span class="n">C</span><span class="o">-&gt;</span><span class="n">B</span><span class="o">-&gt;</span><span class="n">D</span> <span class="o">=</span> <span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="o">+</span><span class="mi">3</span> <span class="o">=</span> <span class="mi">6</span><span class="err">，</span><span class="n">比之前的7要好</span>
<span class="n">堆</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">6</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">distances</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">:</span> <span class="mi">6</span><span class="p">}</span>

<span class="c1"># 第4步：弹出(6, 'D')，D没有邻居，算法结束
</span></code></pre></div></div>]]></content><author><name></name></author><category term="computer-science"/><category term="algorithms"/><category term="data-structures"/><category term="optimization"/><summary type="html"><![CDATA[探索堆数据结构如何优化最短路径算法，以及它在现实生活中的应用]]></summary></entry><entry><title type="html">Cursor + Foam：DailyLog 打造个人知识与成长闭环</title><link href="https://1587causalai.github.io/blog/2024/dailylog-cursor-foam/" rel="alternate" type="text/html" title="Cursor + Foam：DailyLog 打造个人知识与成长闭环"/><published>2024-11-13T18:30:00+08:00</published><updated>2024-11-13T18:30:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/dailylog-cursor-foam</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/dailylog-cursor-foam/"><![CDATA[<h2 id="背景需求">背景需求</h2> <p>作为一名开发者，我需要一个结合开发环境和思维工具的个人知识管理系统。我特别看重使用 Cursor 作为主要的开发和思考工具，因为它能将编程环境的强大功能与 AI 辅助思考完美结合。</p> <p>核心特性要求:</p> <ul> <li>每日安排：能够规划、记录和回顾每天的工作内容</li> <li>创意激发：通过 Cursor AI 辅助进行 Brainstorm，捕捉和发展想法</li> <li>知识沉淀：将日常记录和头脑风暴的成果形成知识积累</li> </ul> <p>为什么不用现成的解决方案（如飞书文档或时间管理软件）？因为我需要一个能与 Cursor 深度集成的系统。我把 Cursor 视为我的”思维伙伴”，它不仅是开发工具，更是我进行深度思考的助手。通过这个自定义的系统，我可以：</p> <ul> <li>在 Cursor 中快速记录每日计划和总结</li> <li>利用 AI 进行头脑风暴和想法扩展</li> <li>将 Brainstorm 的成果直接转化为开发任务</li> <li>在同一环境中完成从构思到编码的全过程</li> </ul> <h2 id="dailylog-项目介绍">DailyLog 项目介绍</h2> <p>基于上述需求，我开发了 DailyLog 一个面向开发者的个人知识与成长管理系统。它的工作流程是：</p> <ol> <li> <p><strong>每日安排（Daily Planning）</strong></p> <ul> <li>在 journal/YYYY-MM-DD.md 中记录每日计划</li> <li>使用 Cursor AI 辅助制定和优化计划</li> <li>随时记录工作进展和思考</li> </ul> </li> <li> <p><strong>头脑风暴（Brainstorm）</strong></p> <ul> <li>在 journal/brainstorm/ 中记录创意想法</li> <li>与 Cursor AI 进行深度对话，拓展思路</li> </ul> </li> <li> <p><strong>知识整理</strong></p> <ul> <li>用 Foam 的知识图谱可视化想法关联</li> <li>通过标签系统组织内容主题</li> <li>选择性地分享和发布成果</li> </ul> </li> </ol> <h2 id="foam-支持功能">Foam 支持功能</h2> <p><a href="https://github.com/foambubble/foam">Foam</a> 作为基础框架，为上述工作流提供了必要的工具支持：</p> <ol> <li>Daily note：支持每日计划和记录</li> <li>笔记链接：通过 [[wikilinks]] 快速关联计划、想法和项目</li> <li>知识图谱： <ul> <li>通过 “Foam: Show Graph” 命令可视化笔记关系</li> <li>支持标签探索</li> <li>提供反向链接功能</li> <li>帮助发现想法间的潜在联系</li> </ul> </li> </ol>]]></content><author><name></name></author><category term="tools"/><category term="cursor"/><category term="dailylog"/><category term="knowledge-management"/><summary type="html"><![CDATA[一个面向开发者的日程安排和知识管理系统]]></summary></entry><entry><title type="html">Overleaf + Cursor 论文写作指南</title><link href="https://1587causalai.github.io/blog/2024/overleaf-cursor/" rel="alternate" type="text/html" title="Overleaf + Cursor 论文写作指南"/><published>2024-11-04T10:30:00+08:00</published><updated>2024-11-04T10:30:00+08:00</updated><id>https://1587causalai.github.io/blog/2024/overleaf-cursor</id><content type="html" xml:base="https://1587causalai.github.io/blog/2024/overleaf-cursor/"><![CDATA[<p>在学术写作中，Overleaf是强大的在线LaTeX编辑平台，Cursor是智能的本地编辑器。将两者结合使用，可以让您的论文写作体验更加流畅和高效。</p> <p>详情参见 <a href="https://github.com/1587causalai/LLM-Quickstart-Guide/blob/main/cursor/overleaf-cursor-guide.md">Overleaf + Cursor 论文写作指南</a></p> <p>在我的想象中, 人工智能会逐渐取代学术写作中的重复性工作, 人们只需要负责决策, 反馈, 和创新.</p> <h2 id="2024-12-23-更新">2024-12-23 更新</h2> <p>这种方式有很大的局限性, 就是每次登录起来都比较复杂，我感觉还是用 github 版本管理, docsify 来写博客比较好。这个以后成为我的最主要的工作方式了。</p>]]></content><author><name></name></author><category term="tools"/><category term="cursor"/><summary type="html"><![CDATA[使用 Overleaf 和 Cursor AI 提升学术写作效率]]></summary></entry></feed>